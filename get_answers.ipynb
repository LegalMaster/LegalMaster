{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import pickle\n",
    "import ray\n",
    "#from sklearn \n",
    "import datasets\n",
    "import tqdm\n",
    "import shortuuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import bitsandbytes as bn\n",
    "from tqdm import tqdm\n",
    "\n",
    "## models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "## utils\n",
    "from utils import dataset, model\n",
    "from utils.model import sample_decode, load_tokenizer_and_model, load_tokenizer_and_model_multiple\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/prompt'\n",
    "answer_dir = './data/answers'\n",
    "base_model_dir = '/home/sojungkim2/legalmaster/7Boutput'\n",
    "adapter_1_dir = '/home/sojungkim2/legalmaster/LegalMaster/ChatAdapterTraining/checkpoints'\n",
    "adapter_2_dir = ''\n",
    "model_id = 1\n",
    "#0: llama_legal, 1: llama_chat, 2: llama_chat_legal, 3: llama_legal_chat\n",
    "\n",
    "    # generated = evaluate(args.model_id, args.data_dir, args.answer_dir, args.gpu_num)\n",
    "    # evaluate(args.model_id, args.data_dir, args.answer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data_dir):\n",
    "    # build dataset\n",
    "    path = data_dir\n",
    "\n",
    "    # load file\n",
    "    if os.path.isfile(os.path.join(path, 'prompt.pkl')):\n",
    "        with open(os.path.join(path, 'prompt.pkl'), 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "    else:\n",
    "        dataset_list = build_dataset(['case_hold'], path)\n",
    "        dataset = dataset_list[0]['test']\n",
    "        random.seed(7)\n",
    "        dataset = dataset.shuffle().map(prompt_engineering).select_columns(['question', 'label'])\n",
    "        dataset = dataset.add_column(name = 'idx', column = range(3600))\n",
    "        # save file\n",
    "        with open(os.path.join(path, 'prompt.pkl'), 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_engineering(data_point):\n",
    "\n",
    "    prompt_cands = [\n",
    "    \"Please select the most suitable summary of the legal ruling that accompanies the relevant referenced decisions for the specific case. The following excerpt is from the court's decision.\",\n",
    "    \"Kindly choose the concise summary of the legal ruling that accompanies the relevant referenced decisions applicable to the given case. Provided below is an excerpt from the court decision.\",\n",
    "    \"Please decide on the most appropriate summary of the legal ruling that accompanies the relevant referenced decisions, which are relevant to the given case. Here is an excerpt from the court decision for your consideration.\",\n",
    "    \"Here is an excerpt from the court decision for the case. Please choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Consider the following excerpt from the court decision for the case. Your task is to select the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Given the excerpt from the court decision for the case, your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Please refer to the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case, using the excerpt from the court decision provided below.\",\n",
    "    \"Please review the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Here is the excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\"\n",
    "    ] # generated by Chat-GPT\n",
    "\n",
    "    \n",
    "    _slice = data_point['context'].find('(<HOLDING>)')\n",
    "    excerpt = data_point['context'][:_slice]\n",
    "    choices = [str(n) + f': {c}' for n,c in enumerate(data_point['endings'])]\n",
    "    prompt = random.choice(prompt_cands)\n",
    "    input = prompt + f'\\nExcerpt: {excerpt}' + f'\\nChoices: {choices}'\n",
    "\n",
    "    return {\n",
    "        'question' : input,\n",
    "        'label' : data_point['label']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_answers(tokenizer, model, questions, device_map):\n",
    "\n",
    "    answers = pd.DataFrame(columns = [\"prompt_id\", \"prompt\",\"answer_id\", \"answer\"])\n",
    "\n",
    "    with torch.no_grad(): # inactivates pytorch autograd engine so that gradient is not tracked anymore, saving memory and accelerating speed\n",
    "\n",
    "        for _idx, question in enumerate(tqdm(questions)):\n",
    "            if _idx > 10: \n",
    "                break\n",
    "            prompt = question['question']\n",
    "            input_ids = tokenizer([prompt], return_tensors = 'pt')['input_ids'][:, -1024:].cuda() # index last 1024 tokens of all questions\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "#             print(f'question: {question}')\n",
    "\n",
    "#             outputs = simple_decode(\n",
    "#                 input_ids,\n",
    "#                 model,\n",
    "#                 tokenizer,\n",
    "#                 max_new_tokens = 50,\n",
    "#             )\n",
    "            outputs = sample_decode(\n",
    "                        input_ids,\n",
    "                        model,\n",
    "                        tokenizer,\n",
    "                        max_length = 50,).replace('<s>', '').replace('</s>', '')\n",
    "\n",
    "            ans_id = shortuuid.uuid()\n",
    "            answers = answers.append(\n",
    "                {\n",
    "                    \"prompt_id\" : _idx,\n",
    "                    \"prompt\": question, \n",
    "                    \"answer_id\" : ans_id,\n",
    "                    \"answer\" : outputs,\n",
    "                }\n",
    "                )\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generate(model_id, dataset, answer_path):\n",
    "    questions = dataset.select_columns(['question', 'idx'])\n",
    "\n",
    "    # chunk_size = len(questions) // num_gpus\n",
    "    # ans_handlers = []\n",
    "    \n",
    "    # tokenizer, model, _device = load_tokenizer_and_model(args.base_model_dir, args.adapter_1_dir, load_8bit=True)\n",
    "    if torch.cuda.is_available():\n",
    "        _device = \"cuda\"\n",
    "    else:\n",
    "        _device = \"cpu\"\n",
    "\n",
    "    try:\n",
    "        if torch.backends.mps.is_available():\n",
    "            _device = \"mps\"\n",
    "    except:  # noqa: E722\n",
    "        pass\n",
    "    \n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model_dir)\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model_dir,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map={\"\": 0},\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        adapter_1_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map = {\"\":0}\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    # print(model_id)\n",
    "    # if model_id in [0, 1]:\n",
    "    #     if args.adapter_2_dir:\n",
    "    #         raise ModelIDAdapterMismatchError(\"For model{}, your adapter_2_dir argument should be empty, since only one adapter is necessary\".format(model_id))\n",
    "    #     else:\n",
    "    #         tokenizer, model, _device = load_tokenizer_and_model(args.base_model_dir, args.adapter_1_dir, load_8bit=True)\n",
    "    # if model_id in [2, 3]:\n",
    "    #     if not args.adapter_2_dir:\n",
    "    #         raise ModelIDAdapterMismatchError(\"For model{}, your adapter_2_dir argument should be given, since 2 adapters are necessary\".format(model_id))\n",
    "    #     else:\n",
    "    #        tokenizer, model, _device = load_tokenizer_and_model_multiple(args.base_model_dir, args.adapter_1_dir, args.adapter_2_dir, load_8bit=True)        \n",
    "        \n",
    "    device_map = {\"\":0}\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "#     ddp = world_size != 1\n",
    "#     if ddp:\n",
    "#         device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "#         GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n",
    " \n",
    "    #model = LlamaForCausalLM.from_pretrained('llama', device_map = device_map, torch_dtype = torch.float16)\n",
    "\n",
    "    print(f'Device: {_device}')\n",
    "    answers = get_model_answers(tokenizer, model, questions, device_map)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for question in questions:\n",
    "    #         answers.append(get_model_answers(tokenizer, model, question))\n",
    "    \n",
    "    # answers = datasets.Dataset.from_pandas(answers) # convert to Dataset obj\n",
    "\n",
    "    if not os.path.exists(answer_path):\n",
    "        os.makedirs(answer_path)\n",
    "    # save the answer\n",
    "    with open(os.path.join(answer_path, f'answers_{model_id}.csv'), 'wb') as f:\n",
    "        pickle.dump(answers, f)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53aec281551144b6a8f49f7a97336828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dataset \u001b[39m=\u001b[39m make_dataset(data_dir)\n\u001b[1;32m      3\u001b[0m \u001b[39m# run_generate(model_id, dataset, answer_path, num_gpus)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m run_generate(model_id, dataset, answer_dir)\n",
      "Cell \u001b[0;32mIn[21], line 20\u001b[0m, in \u001b[0;36mrun_generate\u001b[0;34m(model_id, dataset, answer_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tokenizer \u001b[39m=\u001b[39m LlamaTokenizer\u001b[39m.\u001b[39mfrom_pretrained(base_model_dir)\n\u001b[0;32m---> 20\u001b[0m model \u001b[39m=\u001b[39m LlamaForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     21\u001b[0m     base_model_dir,\n\u001b[1;32m     22\u001b[0m     load_in_8bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     23\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[1;32m     24\u001b[0m     device_map\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0\u001b[39;49m},\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     27\u001b[0m     model,\n\u001b[1;32m     28\u001b[0m     adapter_1_dir,\n\u001b[1;32m     29\u001b[0m     torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16,\n\u001b[1;32m     30\u001b[0m     device_map \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m0\u001b[39m}\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda/envs/legalmaster/lib/python3.8/site-packages/transformers/modeling_utils.py:2881\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2871\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2872\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2874\u001b[0m     (\n\u001b[1;32m   2875\u001b[0m         model,\n\u001b[1;32m   2876\u001b[0m         missing_keys,\n\u001b[1;32m   2877\u001b[0m         unexpected_keys,\n\u001b[1;32m   2878\u001b[0m         mismatched_keys,\n\u001b[1;32m   2879\u001b[0m         offload_index,\n\u001b[1;32m   2880\u001b[0m         error_msgs,\n\u001b[0;32m-> 2881\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   2882\u001b[0m         model,\n\u001b[1;32m   2883\u001b[0m         state_dict,\n\u001b[1;32m   2884\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2885\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2886\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2887\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   2888\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   2889\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   2890\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   2891\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   2892\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   2893\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   2894\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   2895\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(load_in_8bit \u001b[39mor\u001b[39;49;00m load_in_4bit),\n\u001b[1;32m   2896\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   2897\u001b[0m     )\n\u001b[1;32m   2899\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   2900\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/anaconda/envs/legalmaster/lib/python3.8/site-packages/transformers/modeling_utils.py:3228\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3218\u001b[0m mismatched_keys \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3219\u001b[0m     state_dict,\n\u001b[1;32m   3220\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   3225\u001b[0m )\n\u001b[1;32m   3227\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[0;32m-> 3228\u001b[0m     new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   3229\u001b[0m         model_to_load,\n\u001b[1;32m   3230\u001b[0m         state_dict,\n\u001b[1;32m   3231\u001b[0m         loaded_keys,\n\u001b[1;32m   3232\u001b[0m         start_prefix,\n\u001b[1;32m   3233\u001b[0m         expected_keys,\n\u001b[1;32m   3234\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3235\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3236\u001b[0m         offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   3237\u001b[0m         state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   3238\u001b[0m         state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   3239\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3240\u001b[0m         is_quantized\u001b[39m=\u001b[39;49mis_quantized,\n\u001b[1;32m   3241\u001b[0m         is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   3242\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3243\u001b[0m     )\n\u001b[1;32m   3244\u001b[0m     error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   3245\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda/envs/legalmaster/lib/python3.8/site-packages/transformers/modeling_utils.py:728\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    725\u001b[0m             fp16_statistics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    727\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m param_name:\n\u001b[0;32m--> 728\u001b[0m             set_module_quantized_tensor_to_device(\n\u001b[1;32m    729\u001b[0m                 model, param_name, param_device, value\u001b[39m=\u001b[39;49mparam, fp16_statistics\u001b[39m=\u001b[39;49mfp16_statistics\n\u001b[1;32m    730\u001b[0m             )\n\u001b[1;32m    732\u001b[0m \u001b[39mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m~/anaconda/envs/legalmaster/lib/python3.8/site-packages/transformers/utils/bitsandbytes.py:101\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, fp16_statistics)\u001b[0m\n\u001b[1;32m     99\u001b[0m     new_value \u001b[39m=\u001b[39m old_value\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    100\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 101\u001b[0m     new_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     new_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda/envs/legalmaster/lib/python3.8/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    248\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# get answers\n",
    "dataset = make_dataset(data_dir)\n",
    "# run_generate(model_id, dataset, answer_path, num_gpus)\n",
    "run_generate(model_id, dataset, answer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_id, data_dir, answer_path):\n",
    "    # define model\n",
    "\n",
    "    model_name = ['llama_legal', 'llama_chat', 'llama_legal_chat', 'llama_chat_legal'][model_id]\n",
    "\n",
    "    # get answers\n",
    "    dataset = make_dataset(data_dir)\n",
    "    # run_generate(model_id, dataset, answer_path, num_gpus)\n",
    "    run_generate(model_id, dataset, answer_path)\n",
    "\n",
    "    with open(os.path.join(answer_path, f'answers_{model_id}.csv'), 'rb') as f:\n",
    "        answers = pickle.load(f).select_columns(['answer', 'idx'])\n",
    "    \n",
    "    labels = dataset.select_columns(['label', 'idx'])\n",
    "\n",
    "    dataset = datasets.concatenate_datasets([answers, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def calculate_metric(dataset):\n",
    "\n",
    "    for i, data in enumerate(dataset):\n",
    "\n",
    "        idx = dataset['idx']\n",
    "        answer = dataset['answer']\n",
    "        label = dataset['label']\n",
    "\n",
    "        incorrect_correct_labels = np.array([0,0,0,0,0,0,0]) # one-hot-encoding: [incorrect, correct, 0,1,2,3,4 (ground truth)]\n",
    "\n",
    "        r = int(label in answer) # incorrect: 0, correct: 1\n",
    "        incorrect_correct_labels[r] += 1\n",
    "        incorrect_correct_labels[label+2] += 1\n",
    "    \n",
    "    return incorrect_correct_labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legalmaster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
