{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9772df7",
   "metadata": {},
   "source": [
    "### 1. Collect / Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d02a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5890ff5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset lex_glue (/home/laal_intern003/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n",
      "Found cached dataset lex_glue (/home/laal_intern003/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n"
     ]
    }
   ],
   "source": [
    "ds_train = load_dataset(\"lex_glue\",\"case_hold\", split = \"train\")\n",
    "ds_valid = load_dataset(\"lex_glue\",\"case_hold\", split = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "819acf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = DatasetDict(\n",
    "   {\"train\": ds_train.shuffle().select(range(5000)),\n",
    "    \"valid\": ds_valid.shuffle().select(range(500))\n",
    "   }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae80257e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context\n",
      "endings\n",
      "label\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets['train'][0]:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45ef22a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d8cd77",
   "metadata": {},
   "source": [
    "- GPT-2, GPT-3에서는 컨텍스트 크기가 각각 1024, 2048. LLaMA도 찾아보니 2048이라 함. 출처)https://news.ycombinator.com/item?id=35186185. 일단은 128 토큰으로 정해보자.\n",
    "- 여기서 중요한 것은, return_overflowing_tokens 옵션을 사용해서 전체 입력을 토큰화 하고 여러 청크로 분할하는 작업이다. 또 return_length 옵션을 사용해 생성된 각 청크의 길이를 자동으로 반환할 것임. 마지막 청크는 드랍시켜줄 것.\n",
    "- context_length를 128 > 2048로 바꿔보면 청크의 개수가 늘어나고, 각 청크의 길이는 줄어들게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "448146f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa9e8c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained('./llama', add_eos_token = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c41c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "    raw_datasets['train'][:5]['context'],\n",
    "    truncation = True,\n",
    "    max_length = context_length,\n",
    "    return_overflowing_tokens = True,\n",
    "    return_length = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "02a1f570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1a2c3106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 11\n",
      "Input chunk lengths: [128, 100, 128, 97, 128, 128, 128, 128, 40, 128, 94]\n",
      "Chunk mapping: [0, 0, 1, 1, 2, 2, 3, 3, 3, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "print(f'Input IDs length: {len(outputs[\"input_ids\"])}')\n",
    "print(f'Input chunk lengths: {(outputs[\"length\"])}')\n",
    "print(f'Chunk mapping: {outputs[\"overflow_to_sample_mapping\"]}') # 어떤 청크가 어떤 입력 샘플에 속하는 지 재구성할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "468d94dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126cf72",
   "metadata": {},
   "source": [
    "- 더 효율적인 data preparation을 위해서는 eos_token_id 토큰을 사용해서 단일 배치 내의 모든 토큰화된 샘플을 결합한 다음 결합된 시퀀스에서 청킹을 수행하는 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1d82f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\n",
    "    './llama',\n",
    "    vocab_size = len(tokenizer),\n",
    "    n_ctx = context_length,\n",
    "    bos_token_id = tokenizer.bos_token_id,\n",
    "    eos_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7b7320dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"./llama\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.29.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc9e5847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c78f6c3eb849b6814a036dc2882869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50278b4d1574f0592791f0b2b25f86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "    element['context'],\n",
    "    truncation = True,\n",
    "    max_length = context_length,\n",
    "    return_overflowing_tokens = True,\n",
    "    return_length = True\n",
    "    )\n",
    "    input_batch = []\n",
    "#     input_batch.append(outputs['input_ids'])\n",
    "#     return {\"input_ids\": input_batch} \n",
    "    for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids) # dropping\n",
    "    return {\"input_ids\": input_batch} \n",
    "    \n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched = True, remove_columns = raw_datasets['train'].column_names\n",
    ") # batched = True: True로 설정되, 호출되는 순간마다 여러 개의 예제로 구성된 하나의 배치(batch)가 한번에 map 함수에 입력됨.배치 크기는 별도로 설정이 가능하며, 디폴트 값은 1000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e79a2881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8442"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[tokenized_datasets['train'][i] for i in range(5)]\n",
    "len(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "adfc3566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43d18375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/laal_intern003/anaconda/envs/legal-master did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/10100/vscode-ipc-3405e988-41a9-43ff-ab2f-298fa0e77142.sock')}\n",
      "  warn(msg)\n",
      "/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ed45b909d745cba3f777a7e92c7bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "#model = LlamaForCausalLM(config, load_in_8bit = True) # from_pretrained()함수를 사용하지 않고 모델을 직접 초기화 함\n",
    "model = LlamaForCausalLM.from_pretrained('./llama', load_in_8bit = True, device_map = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "66009481",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3ec6ebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA size: 6738.4M parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'LLaMA size: {model_size/1000**2:.1f}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a130e",
   "metadata": {},
   "source": [
    "- 우리모델에는 optimize해야 할 67억 3840만개의 parameter가 있다. 학습을 시작하기 전에 배치 생성을 처리할 datacollator를 설정해야 한다.\n",
    "- datacollator가 하는 일: 1) 배치 구성 2) 패딩 3) LM label 생성. 참고로 datacollator는 학습 중에 입력을 자동으로 생성하기 때문에 input_ids를 복제할 필요가 없다.\n",
    "- DataCollatorForLanguageModeling을 사용. 이 모듈은 MLM, CLM을(mlm=False) 모두 지원."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7edf8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>, </s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "print(f'{tokenizer.bos_token}, {tokenizer.eos_token}')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False) # causal language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ebe9bd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=LlamaTokenizerFast(name_or_path='./llama', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b8dc5a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   505,  1303,   278, 22569,  9245, 30010, 29879,   766,  4838,\n",
       "           291,   653,   740,  4251,   408,   972,  5414, 13047,   304,  8820,\n",
       "           393,   526,  1185,   329,  2015,  1891,  1363,   896,   526,   443,\n",
       "          3075,  5008,   284, 29892,   410,  7588,  2580,   491,  1002,  1082,\n",
       "         29892,   470, 13461,   278,  6874,   310,   385,  6221, 30010, 29879,\n",
       "         14329,  3178,   498,  1280,  1383,   666, 19852,   669, 10088,  1466,\n",
       "          3189, 29889,   325, 29889,  3303,  3900, 29892, 29871, 29941, 29945,\n",
       "         29900,   383, 29889, 29941, 29881, 29871, 29906, 29946, 29955, 29892,\n",
       "         29871, 29906, 29945, 29946, 29899, 29945, 29945,   313, 29896,   303,\n",
       "         25079, 29889, 29906, 29900, 29900, 29941, 29897,   313,  1054, 13081,\n",
       "           292,  4251,   467,  2398, 29892,   591,   505,   451, 11527,  3692,\n",
       "           278,   766,  4838,   291,   653,   740,  3682,  4988,   304,  8820,\n",
       "           393,   526,  6467,  5377,   278,  6874,   310,   385],\n",
       "        [    1,   325, 29889,  5556, 29887,   912, 29892, 29871, 29946, 29900,\n",
       "         29953,   317, 29889, 29956, 29889, 29906, 29881, 29871, 29947, 29929,\n",
       "         29955, 29892, 29871, 29947, 29929, 29929,   313, 26887, 29889, 29896,\n",
       "         29929, 29953, 29953,  8106,   450, 13494,   524, 28324,   892,  1716,\n",
       "         12919, 12530,   278, 29871, 29947, 29899, 18721,  6554,  1434,   896,\n",
       "          6153,   304,   278, 29871, 29896, 29906, 29899, 18721,  9500,  1788,\n",
       "           988,   896,   892, 12530,   278, 29871, 29896, 29906, 29899, 18721,\n",
       "          6554,   313, 29875, 29889, 29872,  1696,   278,  1346, 15807,   630,\n",
       "          6554, 30024, 11977,   472,   491,  6674,  5890,  1009, 29871, 29947,\n",
       "         29899, 18721,  6554,   491,   869, 29947, 29945, 29955,   467,  6549,\n",
       "         29892,   727,   471,  9436,   263, 11781,   310,   278, 27656,   322,\n",
       "           263,  8078, 22856,  1546,   278, 13973,   411,  3390,   304,   278,\n",
       "         29871, 29947, 29899, 18721,  6554,   313, 29875, 29889],\n",
       "        [    1, 29871, 29906, 29929,   501, 29889, 29903, 29889, 29907, 29889,\n",
       "         16683, 29871, 29896, 29896, 29946, 29946, 29898, 29890,  5033, 29906,\n",
       "          5033, 29933,   416,   383, 12513,  2994, 29886, 29889,   325, 29889,\n",
       "           379,  3028, 22394, 29892, 29871, 29946, 29929, 29947,   501, 29889,\n",
       "         29903, 29889, 29871, 29945, 29906, 29892, 29871, 29945, 29955, 29899,\n",
       "         29945, 29947, 29892, 29871, 29953, 29896, 29892, 29871, 29896, 29896,\n",
       "         29896,   317, 29889, 29907, 29873, 29889, 29871, 29946, 29900, 29941,\n",
       "         29892, 29871, 29946, 29900, 29955, 29892, 29871, 29896, 29896, 29906,\n",
       "           365, 29889,  3853, 29889, 29906, 29881, 29871, 29941, 29945, 29953,\n",
       "           313, 29896, 29929, 29929, 29900,   467,   450,   316,   331, 29899,\n",
       "           261, 11845,  8128, 29901,  1346,  8139,  2121,   385, 19001, 14169,\n",
       "          3814,  5439,   297,  4004, 29871, 29896, 29900, 29900, 29941, 29898,\n",
       "         29874, 29897,   310,   445,  3611, 29892,   607,   338],\n",
       "        [    1,   263,  5995,  2750,   278,  2553,  7345,   393, 28811,  1434,\n",
       "           278,   844,  3977,   882,   310,   278, 16326,  1090,   445,  3611,\n",
       "         29936, 29871, 29896, 29896,   501, 29889, 29903, 29889, 29907, 29889,\n",
       "         16683, 29871, 29941, 29953, 29906, 29898, 29874, 29897,   313,  7278,\n",
       "         25101,  2715,   467,  1094,   278,  1002,  1082,  9436, 14088, 29892,\n",
       "         16683, 29871, 29941, 29953, 29906, 29898, 29874, 29897,   871, 27111,\n",
       "          1906,  1346,   771,  3905, 21219, 29961, 29879, 29962,  2750,   278,\n",
       "          2553,  7345,  3995,  1074,  3878, 11422,   325, 29889, 11444,   261,\n",
       "           310,   512,  1890,   830,  9947, 29892, 29871, 29955, 29929, 29929,\n",
       "           383, 29889, 29906, 29881, 29871, 29896, 29900, 29929, 29896, 29892,\n",
       "         29871, 29896, 29900, 29929, 29906, 29899, 29929, 29941,   313, 29945,\n",
       "           386, 25079, 29889, 29896, 29929, 29947, 29953,   511, 27999,  1346,\n",
       "         14676,   312, 29961,   292, 29962,   278,  2553,  7345],\n",
       "        [    1,   304,   670,   610,  1772, 12139,   297,   278,  1023, 11405,\n",
       "          7536,   304,  4779, 29871, 29896, 29906, 29892, 29871, 29906, 29900,\n",
       "         29900, 29955, 29892,   322,  7455,  1512,   750,   451,  1063,  2221,\n",
       "           304,  6159,  1075,   472,   670,   610,  1772,  6958,  1353, 29892,\n",
       "          1074,  6892,  3303,  3900,   325, 29889,   838, 29899, 29903,  1114,\n",
       "          4353, 29892, 29871, 29946, 29941, 29906,   383, 29889, 29941, 29881,\n",
       "         29871, 29946, 29896, 29929, 29892, 29871, 29946, 29906, 29945,   313,\n",
       "         29906, 29881, 25079, 29889, 29871, 29906, 29900, 29900, 29945, 29897,\n",
       "           313, 29423,  5281,   393, 16286,  7389,  1156, 17268,   508,   367,\n",
       "         10757,   310, 19861,  2264,   310,  1410,  2782,   310,   393, 17268,\n",
       "           467,  1551,   445,  2407, 29892,   315,  6119,   276, 30010, 29879,\n",
       "          2980,   393,   278, 24114,  2756,   333,   485,   277,   723,   451,\n",
       "         22222, 16269,  4556,  8465,   408,   263,  4383,   310]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[    1,   505,  1303,   278, 22569,  9245, 30010, 29879,   766,  4838,\n",
       "           291,   653,   740,  4251,   408,   972,  5414, 13047,   304,  8820,\n",
       "           393,   526,  1185,   329,  2015,  1891,  1363,   896,   526,   443,\n",
       "          3075,  5008,   284, 29892,   410,  7588,  2580,   491,  1002,  1082,\n",
       "         29892,   470, 13461,   278,  6874,   310,   385,  6221, 30010, 29879,\n",
       "         14329,  3178,   498,  1280,  1383,   666, 19852,   669, 10088,  1466,\n",
       "          3189, 29889,   325, 29889,  3303,  3900, 29892, 29871, 29941, 29945,\n",
       "         29900,   383, 29889, 29941, 29881, 29871, 29906, 29946, 29955, 29892,\n",
       "         29871, 29906, 29945, 29946, 29899, 29945, 29945,   313, 29896,   303,\n",
       "         25079, 29889, 29906, 29900, 29900, 29941, 29897,   313,  1054, 13081,\n",
       "           292,  4251,   467,  2398, 29892,   591,   505,   451, 11527,  3692,\n",
       "           278,   766,  4838,   291,   653,   740,  3682,  4988,   304,  8820,\n",
       "           393,   526,  6467,  5377,   278,  6874,   310,   385],\n",
       "        [    1,   325, 29889,  5556, 29887,   912, 29892, 29871, 29946, 29900,\n",
       "         29953,   317, 29889, 29956, 29889, 29906, 29881, 29871, 29947, 29929,\n",
       "         29955, 29892, 29871, 29947, 29929, 29929,   313, 26887, 29889, 29896,\n",
       "         29929, 29953, 29953,  8106,   450, 13494,   524, 28324,   892,  1716,\n",
       "         12919, 12530,   278, 29871, 29947, 29899, 18721,  6554,  1434,   896,\n",
       "          6153,   304,   278, 29871, 29896, 29906, 29899, 18721,  9500,  1788,\n",
       "           988,   896,   892, 12530,   278, 29871, 29896, 29906, 29899, 18721,\n",
       "          6554,   313, 29875, 29889, 29872,  1696,   278,  1346, 15807,   630,\n",
       "          6554, 30024, 11977,   472,   491,  6674,  5890,  1009, 29871, 29947,\n",
       "         29899, 18721,  6554,   491,   869, 29947, 29945, 29955,   467,  6549,\n",
       "         29892,   727,   471,  9436,   263, 11781,   310,   278, 27656,   322,\n",
       "           263,  8078, 22856,  1546,   278, 13973,   411,  3390,   304,   278,\n",
       "         29871, 29947, 29899, 18721,  6554,   313, 29875, 29889],\n",
       "        [    1, 29871, 29906, 29929,   501, 29889, 29903, 29889, 29907, 29889,\n",
       "         16683, 29871, 29896, 29896, 29946, 29946, 29898, 29890,  5033, 29906,\n",
       "          5033, 29933,   416,   383, 12513,  2994, 29886, 29889,   325, 29889,\n",
       "           379,  3028, 22394, 29892, 29871, 29946, 29929, 29947,   501, 29889,\n",
       "         29903, 29889, 29871, 29945, 29906, 29892, 29871, 29945, 29955, 29899,\n",
       "         29945, 29947, 29892, 29871, 29953, 29896, 29892, 29871, 29896, 29896,\n",
       "         29896,   317, 29889, 29907, 29873, 29889, 29871, 29946, 29900, 29941,\n",
       "         29892, 29871, 29946, 29900, 29955, 29892, 29871, 29896, 29896, 29906,\n",
       "           365, 29889,  3853, 29889, 29906, 29881, 29871, 29941, 29945, 29953,\n",
       "           313, 29896, 29929, 29929, 29900,   467,   450,   316,   331, 29899,\n",
       "           261, 11845,  8128, 29901,  1346,  8139,  2121,   385, 19001, 14169,\n",
       "          3814,  5439,   297,  4004, 29871, 29896, 29900, 29900, 29941, 29898,\n",
       "         29874, 29897,   310,   445,  3611, 29892,   607,   338],\n",
       "        [    1,   263,  5995,  2750,   278,  2553,  7345,   393, 28811,  1434,\n",
       "           278,   844,  3977,   882,   310,   278, 16326,  1090,   445,  3611,\n",
       "         29936, 29871, 29896, 29896,   501, 29889, 29903, 29889, 29907, 29889,\n",
       "         16683, 29871, 29941, 29953, 29906, 29898, 29874, 29897,   313,  7278,\n",
       "         25101,  2715,   467,  1094,   278,  1002,  1082,  9436, 14088, 29892,\n",
       "         16683, 29871, 29941, 29953, 29906, 29898, 29874, 29897,   871, 27111,\n",
       "          1906,  1346,   771,  3905, 21219, 29961, 29879, 29962,  2750,   278,\n",
       "          2553,  7345,  3995,  1074,  3878, 11422,   325, 29889, 11444,   261,\n",
       "           310,   512,  1890,   830,  9947, 29892, 29871, 29955, 29929, 29929,\n",
       "           383, 29889, 29906, 29881, 29871, 29896, 29900, 29929, 29896, 29892,\n",
       "         29871, 29896, 29900, 29929, 29906, 29899, 29929, 29941,   313, 29945,\n",
       "           386, 25079, 29889, 29896, 29929, 29947, 29953,   511, 27999,  1346,\n",
       "         14676,   312, 29961,   292, 29962,   278,  2553,  7345],\n",
       "        [    1,   304,   670,   610,  1772, 12139,   297,   278,  1023, 11405,\n",
       "          7536,   304,  4779, 29871, 29896, 29906, 29892, 29871, 29906, 29900,\n",
       "         29900, 29955, 29892,   322,  7455,  1512,   750,   451,  1063,  2221,\n",
       "           304,  6159,  1075,   472,   670,   610,  1772,  6958,  1353, 29892,\n",
       "          1074,  6892,  3303,  3900,   325, 29889,   838, 29899, 29903,  1114,\n",
       "          4353, 29892, 29871, 29946, 29941, 29906,   383, 29889, 29941, 29881,\n",
       "         29871, 29946, 29896, 29929, 29892, 29871, 29946, 29906, 29945,   313,\n",
       "         29906, 29881, 25079, 29889, 29871, 29906, 29900, 29900, 29945, 29897,\n",
       "           313, 29423,  5281,   393, 16286,  7389,  1156, 17268,   508,   367,\n",
       "         10757,   310, 19861,  2264,   310,  1410,  2782,   310,   393, 17268,\n",
       "           467,  1551,   445,  2407, 29892,   315,  6119,   276, 30010, 29879,\n",
       "          2980,   393,   278, 24114,  2756,   333,   485,   277,   723,   451,\n",
       "         22222, 16269,  4556,  8465,   408,   263,  4383,   310]])}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator([tokenized_datasets['train'][i] for i in range(5)])\n",
    "# input_ids만 넣어줬는데도 attention mask labels이 붙은 채로 텐서의 형태로 통일되어 리턴시켜줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "00009a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets['train'][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f'{key} shape: {out[key].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30436b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "if not os.path.exists('./clmTutorial'):\n",
    "    os.mkdir('./clmTutorial')\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['valid'],\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 32,\n",
    "        per_device_eval_batch_size = 32,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 100,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 0.0002,\n",
    "        logging_steps = 20,\n",
    "        evaluation_strategy = \"steps\",\n",
    "        save_strategy = \"steps\",\n",
    "        eval_steps = 200,\n",
    "        save_steps = 200,\n",
    "        output_dir = './clmTutorial',\n",
    "        save_total_limit = 100,\n",
    "        load_best_model_at_end = True,\n",
    "        ),\n",
    "    data_collator = data_collator\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2f841fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 8442\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 815\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ebb45c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [131/131 08:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=131, training_loss=1.661837279341603, metrics={'train_runtime': 524.7868, 'train_samples_per_second': 15.972, 'train_steps_per_second': 0.25, 'total_flos': 4.253395521739162e+16, 'train_loss': 1.661837279341603, 'epoch': 1.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "103ff6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py:1690: UserWarning: You are calling `save_pretrained` to a 8-bit converted model you may likely encounter unexepected behaviors. If you want to save 8-bit models, make sure to have `bitsandbytes>0.37.2` installed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./clmTutorial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbd5a2",
   "metadata": {},
   "source": [
    "### 2. Evaluate the model through pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f47b7631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73925111f5424431986a45955b1127b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e233ed012adc477bb29656f431716337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601571328f3f4574bbcc95ed2e32d937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ff3031f0cf4034a13b44def0e63400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f73c881ad248f2a08239bcb3fd984e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4f6cd9f7f54cbcbc841d48a697b3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "device = torch.device('cuda')\n",
    "pipe = pipeline(\n",
    "    'text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ef71367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'you suck at playing games. I wouldn\\'t call myself a \"player\" if I didn\\'t care for making a living as a gamer. I know I\\'m not great at doing that, but when I\\'m ready, I can do my job.'}]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(['hi, my name is Lora'])\n",
    "pipe(['you suck at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0eec5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Please review the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\\nExcerpt: been very emphatic that, except in cases where only one inference can be drawn from the facts, negligence, proximate cause, and foreseeability are questions of fact for the jury. See St. Clair v. Denny, 245 Kan. 414, 781 P.2d 1043, 1047 (1989); Baker v. City of Garden City, 240 Kan. 554, 731 P.2d 278, 281 (1987); Gard, 400 P.2d at 1000, 1002; Rowell, 176 P.2d at 595. As might be expected in an area of the law in which so much depends on the factual scenario of the particular case, the holdings of the Kansas cases that address causation are not clearly dispositive of the issue. However, we find that those cases whose facts most closely approximate those of the instant case support the conclusion that the district court improperly granted summary judgment. See Steele, 327 P.2d at 1065 \\nChoices: ['0: holding that in a  1983 action issue of probable cause is for the jury', '1: recognizing the cause of action', '2: recognizing the availability of a cause of action by manufacturer against supplier under consumer fraud act', '3: recognizing cause of action', '4: holding that demurrer improperly granted because dropping bottle of flammable liquid was reasonably probable such that it was not an intervening cause in action against bottle manufacturer']\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bdd8ff07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  3529,  9076,   278,  1494,   429,  2265,   415,   515,   278,\n",
       "          8973, 10608,   363,   278,  1206, 29889,  3575,  3414,   338,   304,\n",
       "          6755,   278,  1556,  8210,  3273, 15837,   310,   278, 11706,   364,\n",
       "         19478,   393, 18509,   583,   278, 16180,  1602, 12112,  8018,   304,\n",
       "           278,  1206, 29889,    13,  1252,  2265,   415, 29901,  1063,  1407,\n",
       "           953,   561,  2454,   393, 29892,  5174,   297,  4251,   988,   871,\n",
       "           697, 27262,   508,   367, 12061,   515,   278, 17099, 29892,  3480,\n",
       "          3473,   663, 29892, 23203,   403,  4556, 29892,   322,   363,   968,\n",
       "         29872,  3097,   526,  5155,   310,  2114,   363,   278,   432,  2857,\n",
       "         29889,  2823,   624, 29889,  6015,   381,   325, 29889,  3384,  1460,\n",
       "         29892, 29871, 29906, 29946, 29945, 11720, 29889, 29871, 29946, 29896,\n",
       "         29946, 29892, 29871, 29955, 29947, 29896,   349, 29889, 29906, 29881,\n",
       "         29871, 29896, 29900, 29946, 29941, 29892, 29871, 29896, 29900, 29946,\n",
       "         29955,   313, 29896, 29929, 29947, 29929,   416, 22652,   325, 29889,\n",
       "          4412,   310, 19906,  4412, 29892, 29871, 29906, 29946, 29900, 11720,\n",
       "         29889, 29871, 29945, 29945, 29946, 29892, 29871, 29955, 29941, 29896,\n",
       "           349, 29889, 29906, 29881, 29871, 29906, 29955, 29947, 29892, 29871,\n",
       "         29906, 29947, 29896,   313, 29896, 29929, 29947, 29955,   416, 15971,\n",
       "         29892, 29871, 29946, 29900, 29900,   349, 29889, 29906, 29881,   472,\n",
       "         29871, 29896, 29900, 29900, 29900, 29892, 29871, 29896, 29900, 29900,\n",
       "         29906, 29936, 11438,   514, 29892, 29871, 29896, 29955, 29953,   349,\n",
       "         29889, 29906, 29881,   472, 29871, 29945, 29929, 29945, 29889,  1094,\n",
       "          1795,   367,  3806,   297,   385,  4038,   310,   278,  4307,   297,\n",
       "           607,   577,  1568,  7111,   373,   278,  2114,   950, 10483,   310,\n",
       "           278,  3153,  1206, 29892,   278,  4808,   886,   310,   278, 20029,\n",
       "          4251,   393,  3211,  3269,   362,   526,   451,  9436, 11549,  3321,\n",
       "           310,   278,  2228, 29889,  2398, 29892,   591,  1284,   393,  1906,\n",
       "          4251,  5069, 17099,  1556, 16467, 26368,  1906,   310,   278, 14426,\n",
       "          1206,  2304,   278, 15997,   393,   278,  6474,  8973,  4857,   546,\n",
       "           368, 16896, 15837, 24284, 29889,  2823,  2443,  6146, 29892, 29871,\n",
       "         29941, 29906, 29955,   349, 29889, 29906, 29881,   472, 29871, 29896,\n",
       "         29900, 29953, 29945, 29871,    13, 15954,  1575, 29901,  6024, 29900,\n",
       "         29901, 13587,   393,   297,   263,   259, 29896, 29929, 29947, 29941,\n",
       "          3158,  2228,   310, 16269,  4556,   338,   363,   278,   432,  2857,\n",
       "           742,   525, 29896, 29901,  5936,  5281,   278,  4556,   310,  3158,\n",
       "           742,   525, 29906, 29901,  5936,  5281,   278, 20847,  3097,   310,\n",
       "           263,  4556,   310,  3158,   491, 12012,  9945,  2750,  1462,  4926,\n",
       "          1090, 21691,  5227,   566,  1044,   742,   525, 29941, 29901,  5936,\n",
       "          5281,  4556,   310,  3158,   742,   525, 29946, 29901, 13587,   393,\n",
       "          1261,  1038,   261,  4857,   546,   368, 16896,  1363,  4441,  3262,\n",
       "         18046,   280,   310,  1652,  4850,   519, 23904,   471,  2769,  2197,\n",
       "         16269,  1316,   393,   372,   471,   451,   385, 26314,   292,  4556,\n",
       "           297,  3158,  2750, 18046,   280, 12012,  9945,  2033]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3064d049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Considering the given context, '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens = True, clean_up_tokenization_sapces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59e897f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 10056,   292,   278,  2183,  3030, 29892, 29871,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9e1e291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> Please review the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\\nExcerpt: been very emphatic that, except in cases where only one inference can be drawn from the facts, negligence, proximate cause, and foreseeability are questions of fact for the jury. See St. Clair v. Denny, 245 Kan. 414, 781 P.2d 1043, 1047 (1989); Baker v. City of Garden City, 240 Kan. 554, 731 P.2d 278, 281 (1987); Gard, 400 P.2d at 1000, 1002; Rowell, 176 P.2d at 595. As might be expected in an area of the law in which so much depends on the factual scenario of the particular case, the holdings of the Kansas cases that address causation are not clearly dispositive of the issue. However, we find that those cases whose facts most closely approximate those of the instant case support the conclusion that the district court improperly granted summary judgment. See Steele, 327 P.2d at 1065 \\nChoices: ['0: holding that in a  1983 action issue of probable cause is for the jury', '1: recognizing the cause of action', '2: recognizing the availability of a cause of action by manufacturer against supplier under consumer fraud act', '3: recognizing cause of action', '4: holding that demurrer improperly granted because dropping bottle of flammable liquid was reasonably probable such that it was not an intervening cause in action against bottle manufacturer']\\nPlease review the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\\nExcerpt: been very emphatic that, except in cases where only one inference can be drawn from the facts, negligence, proximate cause, and foreseeability are questions of fact for the jury. See St. Clair v. Denny, 245 Kan. 414, 781 P.2d 1043, 1047 (1989); Baker v. City of Garden City, 240 Kan. 554, 731 P.2d 278, 281 (1987); Gard, 400 P.2d at 1000, 1002; Rowell, 176 P.2d at 595. As might be expected in an area of the law in which so much depends on the factual scenario of the particular case, the holdings of the Kansas cases that address causation are not clearly dispositive of the issue. However, we find that those cases whose facts most closely approximate those of the instant case support the conclusion that the district court improperly granted summary judgment. See Steele, 327 P.2d at 1065 \\nChoices: ['0: holding that in a  1983 action issue of probable cause is for the jury', '1: recognizing the cause of action', '2: recognizing the availability of a cause of action by manufacturer against supplier under consumer\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_l = LlamaForCausalLM.from_pretrained('./llama')\n",
    "generate_ids = model_l.generate(inputs.input_ids, max_length = 800)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_token = True, clean_up_tokenization_spaces = False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f25ca1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  3529,  9076,   278,  1494,   429,  2265,   415,   515,   278,\n",
       "          8973, 10608,   363,   278,  1206, 29889,  3575,  3414,   338,   304,\n",
       "          6755,   278,  1556,  8210,  3273, 15837,   310,   278, 11706,   364,\n",
       "         19478,   393, 18509,   583,   278, 16180,  1602, 12112,  8018,   304,\n",
       "           278,  1206, 29889,    13,  1252,  2265,   415, 29901,  1063,  1407,\n",
       "           953,   561,  2454,   393, 29892,  5174,   297,  4251,   988,   871,\n",
       "           697, 27262,   508,   367, 12061,   515,   278, 17099, 29892,  3480,\n",
       "          3473,   663, 29892, 23203,   403,  4556, 29892,   322,   363,   968,\n",
       "         29872,  3097,   526,  5155,   310,  2114,   363,   278,   432,  2857,\n",
       "         29889,  2823,   624, 29889,  6015,   381,   325, 29889,  3384,  1460,\n",
       "         29892, 29871, 29906, 29946, 29945, 11720, 29889, 29871, 29946, 29896,\n",
       "         29946, 29892, 29871, 29955, 29947, 29896,   349, 29889, 29906, 29881,\n",
       "         29871, 29896, 29900, 29946, 29941, 29892, 29871, 29896, 29900, 29946,\n",
       "         29955,   313, 29896, 29929, 29947, 29929,   416, 22652,   325, 29889,\n",
       "          4412,   310, 19906,  4412, 29892, 29871, 29906, 29946, 29900, 11720,\n",
       "         29889, 29871, 29945, 29945, 29946, 29892, 29871, 29955, 29941, 29896,\n",
       "           349, 29889, 29906, 29881, 29871, 29906, 29955, 29947, 29892, 29871,\n",
       "         29906, 29947, 29896,   313, 29896, 29929, 29947, 29955,   416, 15971,\n",
       "         29892, 29871, 29946, 29900, 29900,   349, 29889, 29906, 29881,   472,\n",
       "         29871, 29896, 29900, 29900, 29900, 29892, 29871, 29896, 29900, 29900,\n",
       "         29906, 29936, 11438,   514, 29892, 29871, 29896, 29955, 29953,   349,\n",
       "         29889, 29906, 29881,   472, 29871, 29945, 29929, 29945, 29889,  1094,\n",
       "          1795,   367,  3806,   297,   385,  4038,   310,   278,  4307,   297,\n",
       "           607,   577,  1568,  7111,   373,   278,  2114,   950, 10483,   310,\n",
       "           278,  3153,  1206, 29892,   278,  4808,   886,   310,   278, 20029,\n",
       "          4251,   393,  3211,  3269,   362,   526,   451,  9436, 11549,  3321,\n",
       "           310,   278,  2228, 29889,  2398, 29892,   591,  1284,   393,  1906,\n",
       "          4251,  5069, 17099,  1556, 16467, 26368,  1906,   310,   278, 14426,\n",
       "          1206,  2304,   278, 15997,   393,   278,  6474,  8973,  4857,   546,\n",
       "           368, 16896, 15837, 24284, 29889,  2823,  2443,  6146, 29892, 29871,\n",
       "         29941, 29906, 29955,   349, 29889, 29906, 29881,   472, 29871, 29896,\n",
       "         29900, 29953, 29945, 29871,    13, 15954,  1575, 29901,  6024, 29900,\n",
       "         29901, 13587,   393,   297,   263,   259, 29896, 29929, 29947, 29941,\n",
       "          3158,  2228,   310, 16269,  4556,   338,   363,   278,   432,  2857,\n",
       "           742,   525, 29896, 29901,  5936,  5281,   278,  4556,   310,  3158,\n",
       "           742,   525, 29906, 29901,  5936,  5281,   278, 20847,  3097,   310,\n",
       "           263,  4556,   310,  3158,   491, 12012,  9945,  2750,  1462,  4926,\n",
       "          1090, 21691,  5227,   566,  1044,   742,   525, 29941, 29901,  5936,\n",
       "          5281,  4556,   310,  3158,   742,   525, 29946, 29901, 13587,   393,\n",
       "          1261,  1038,   261,  4857,   546,   368, 16896,  1363,  4441,  3262,\n",
       "         18046,   280,   310,  1652,  4850,   519, 23904,   471,  2769,  2197,\n",
       "         16269,  1316,   393,   372,   471,   451,   385, 26314,   292,  4556,\n",
       "           297,  3158,  2750, 18046,   280, 12012,  9945,  2033]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2cbae0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "# pipeline\n",
    "def make_dataset(data_dir):\n",
    "    # build dataset\n",
    "    path = data_dir\n",
    "\n",
    "    # load file\n",
    "    if os.path.isfile(os.path.join(path, 'prompt.pkl')):\n",
    "        with open(os.path.join(path, 'prompt.pkl'), 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "    else:\n",
    "        dataset_list = build_dataset(['case_hold'], path)\n",
    "        dataset = dataset_list[0]['test']\n",
    "        random.seed(7)\n",
    "        dataset = dataset.shuffle().map(prompt_engineering).select_columns(['question', 'label'])\n",
    "        \n",
    "        # save file\n",
    "        with open(os.path.join(path, 'prompt.pkl'), 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def prompt_engineering(data_point):\n",
    "\n",
    "    prompt_cands = [\n",
    "    \"Please select the most suitable summary of the legal ruling that accompanies the relevant referenced decisions for the specific case. The following excerpt is from the court's decision.\",\n",
    "    \"Kindly choose the concise summary of the legal ruling that accompanies the relevant referenced decisions applicable to the given case. Provided below is an excerpt from the court decision.\",\n",
    "    \"Please decide on the most appropriate summary of the legal ruling that accompanies the relevant referenced decisions, which are relevant to the given case. Here is an excerpt from the court decision for your consideration.\",\n",
    "    \"Here is an excerpt from the court decision for the case. Please choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Consider the following excerpt from the court decision for the case. Your task is to select the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Given the excerpt from the court decision for the case, your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Please refer to the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case, using the excerpt from the court decision provided below.\",\n",
    "    \"Please review the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Here is the excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\"\n",
    "    ] # generated by Chat-GPT\n",
    "\n",
    "    \n",
    "    _slice = data_point['context'].find('(<HOLDING>)')\n",
    "    excerpt = data_point['context'][:_slice]\n",
    "    choices = [str(n) + f': {c}' for n,c in enumerate(data_point['endings'])]\n",
    "    prompt = random.choice(prompt_cands)\n",
    "    input = prompt + f'\\nExcerpt: {excerpt}' + f'\\nChoices: {choices}'\n",
    "\n",
    "    return {\n",
    "        'question' : input,\n",
    "        'label' : data_point['label']\n",
    "    }\n",
    "\n",
    "@ray.remote(num_gpus = 1)\n",
    "def tokenize(question, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "@ray.remote(num_gpus = 1)\n",
    "@torch.inference_mode()\n",
    "def get_model_answers(model_id, inputs): # questions: {'questions': input}\n",
    "\n",
    "    base_path = './llama'\n",
    "\n",
    "    # load tokenizer, model\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        base_path,\n",
    "        add_eos_token = True # add end-of-sentence token\n",
    "    )\n",
    "\n",
    "    adapter_names = ['llama_legal', 'llama_chat', 'llama_legal_chat', 'llama_chat_legal']\n",
    "    adapter_name = adapter_names[model_id]\n",
    "    adapter_path = './adapter/' + adapter_name\n",
    "\n",
    "    base_model = LlamaForCausalLM.from_pretrained(\n",
    "        base_path,\n",
    "        device_map = 'auto',\n",
    "        low_cpu_mem_usage = True,\n",
    "        torch_dtype = torch.float16\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        adapter_path,\n",
    "        torch_dtype = torch.float16,\n",
    "    )\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    # get answers\n",
    "    output_ids = model.generate(\n",
    "        torch.as_tensor(inputs.input_ids).cuda(),\n",
    "        max_gen_len = 0.8,\n",
    "        temperature = 0.95, # float\n",
    "    )[0][len(inputs.input_ids):] # only take the answer, not the question itself\n",
    "\n",
    "    answers = tokenizer.decode(output_ids)\n",
    "\n",
    "    return answers\n",
    "\n",
    "def calculate_metric(answers, labels):\n",
    "    \n",
    "    incorrect_correct = torch.tensor([0,0])\n",
    "    labels = torch.tensor([0,0,0,0,0])\n",
    "\n",
    "    for answer, label in enumerate(answers, labels):\n",
    "        r = int(label in answer) # incorrect: 0, correct: 1 \n",
    "        incorrect_correct[r] += 1\n",
    "        labels[label] += 1\n",
    "    \n",
    "    return incorrect_correct, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model_name = ['llama_legal', 'llama_chat', 'llama_legal_chat', 'llama_chat_legal'][0]\n",
    "# split question into num_gpus files\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    base_path,\n",
    "    add_eos_token = True # add end-of-sentence token\n",
    ")\n",
    "\n",
    "inputs = tokenize(datasets['question'])\n",
    "labels = datasets['label']\n",
    "\n",
    "chunk_size = len(datasets['label']) // num_gpus\n",
    "\n",
    "for i in range(0, len(datasets['label']), chunk_size) :\n",
    "    get_model_answers(model_id, inputs).remote()\n",
    "\n",
    "\n",
    "results, labels = calculate_metric(answers, labels).to_list()\n",
    "accuracy = results[1]/(results[0]+results[1])\n",
    "\n",
    "print(f'The accuracy of the {model_name} is : {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6253f0d",
   "metadata": {},
   "source": [
    "### 3. ray module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd39b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from dataset import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa80a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/prompt.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d55d85c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'question'],\n",
       "    num_rows: 3600\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2338091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus = 2)\n",
    "def tokenize(question, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        max_length = 1024,\n",
    "        padding = \"max_length\"\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ddfb9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 15:01:44,427\tINFO worker.py:1636 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.16</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.5.0</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.16', ray_version='2.5.0', ray_commit='586c376e0769082cb5cfa1333e8264a5fa6b73ec', address_info={'node_ip_address': '192.168.0.104', 'raylet_ip_address': '192.168.0.104', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-06-11_15-01-42_135545_1530658/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-06-11_15-01-42_135545_1530658/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2023-06-11_15-01-42_135545_1530658', 'metrics_export_port': 63973, 'gcs_address': '192.168.0.104:63284', 'address': '192.168.0.104:63284', 'dashboard_agent_listen_port': 52365, 'node_id': '0f8fcbfcf788142717a4c512a0b55a0daf54082058cbd5cf80c8f016'})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db974654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e65a19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc76f58fede4ae9a571257b01ec3af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define tokenizer, model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    './llama',\n",
    "    add_eos_token = True # add end-of-sentence token\n",
    ")\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    './llama',\n",
    "    device_map = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b7b31196",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ray.get(tokenize.remote(dataset['question'], tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57452e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs['input_ids'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34346f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "316feb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85729557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_id, datasets, answer_path, num_gpus):\n",
    "    # define model\n",
    "    model_name = ['llama_legal', 'llama_chat', 'llama_legal_chat', 'llama_chat_legal'][model_id]\n",
    "    \n",
    "    # split question into num_gpus files\n",
    "    inputs = tokenize(datasets['question'])\n",
    "    labels = datasets['label']\n",
    "\n",
    "    chunk_size = len(datasets['label']) // num_gpus\n",
    "    \n",
    "    for i in range(0, len(datasets['label']), chunk_size) :\n",
    "        get_model_answers(model_id, inputs).remote()\n",
    "\n",
    "\n",
    "    results, labels = calculate_metric(answers, labels).to_list()\n",
    "    accuracy = results[1]/(results[0]+results[1])\n",
    "\n",
    "    print(f'The accuracy of the {model_name} is : {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0e91059",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = ray.put(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d88720f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000008000000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e67508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "past_datetime = datetime.datetime.now()\n",
    "current_datetime = datetime.datetime.now()\n",
    "time = current_datetime - past_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fec50d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 15:02:40,351\tINFO worker.py:1636 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.16</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.5.0</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.16', ray_version='2.5.0', ray_commit='586c376e0769082cb5cfa1333e8264a5fa6b73ec', address_info={'node_ip_address': '192.168.0.104', 'raylet_ip_address': '192.168.0.104', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-06-11_15-02-37_076163_1530658/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-06-11_15-02-37_076163_1530658/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2023-06-11_15-02-37_076163_1530658', 'metrics_export_port': 52633, 'gcs_address': '192.168.0.104:52521', 'address': '192.168.0.104:52521', 'dashboard_agent_listen_port': 52365, 'node_id': '477ed3a8ac8cccf76112afab9460be693cd1017e2e202cdbdac272bd'})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "007bd066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "00ff8f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a414460c12de45b6bd281d1ca73b87cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "# adapter_names = ['llama_legal', 'llama_chat', 'llama_legal_chat', 'llama_chat_legal']\n",
    "# adapter_name = adapter_names[model_id]\n",
    "adapter_path = './checkpoints_unmasked'\n",
    "\n",
    "base_path = './llama'\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    base_path,\n",
    "    device_map = 'auto',\n",
    "    low_cpu_mem_usage = True,\n",
    "    torch_dtype = torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    torch_dtype = torch.float16,\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8e7fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "# @ray.remote(num_gpus = 2)\n",
    "# @torch.inference_mode()\n",
    "def get_model_answers(model_id, input_ids, tokenizer): # questions: {'questions': input}\n",
    "\n",
    "\n",
    "    \n",
    "    # get answers\n",
    "    output_ids = model.generate(\n",
    "        torch.as_tensor(input_ids).cuda(),\n",
    "        max_new_tokens = 1024,\n",
    "        temperature = 0.95, # float\n",
    "    )[0][len(inputs.input_ids):] # only take the answer, not the question itself\n",
    "\n",
    "    answers = tokenizer.decode(output_ids)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "341d31ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81636bf880d04843b421f307ede99895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answers \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[81], line 19\u001b[0m, in \u001b[0;36mget_model_answers\u001b[0;34m(model_id, input_ids, tokenizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./llama\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m base_model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     14\u001b[0m     base_path,\n\u001b[1;32m     15\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     low_cpu_mem_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# get answers\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/peft/peft_model.py:180\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(model, config, adapter_name)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m model\u001b[38;5;241m.\u001b[39mload_adapter(model_id, adapter_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/peft/peft_model.py:662\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, peft_config: PeftConfig, adapter_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/peft/peft_model.py:99\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name] \u001b[38;5;241m=\u001b[39m peft_config\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mPEFT_TYPE_TO_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/peft/tuners/lora.py:154\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m[\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/peft/tuners/lora.py:161\u001b[0m, in \u001b[0;36mLoraModel.add_adapter\u001b[0;34m(self, adapter_name, config)\u001b[0m\n\u001b[1;32m    159\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_lora_config(config, model_config)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for all adapters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/peft/tuners/lora.py:250\u001b[0m, in \u001b[0;36mLoraModel._find_and_replace\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently, only `torch.nn.Linear` and `Conv1D` are supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m                     )\n\u001b[0;32m--> 250\u001b[0m                 new_module \u001b[38;5;241m=\u001b[39m \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_module(parent, target_name, new_module, target)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/peft/tuners/lora.py:510\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, adapter_name, in_features, out_features, r, lora_alpha, lora_dropout, fan_in_fan_out, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    499\u001b[0m     adapter_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    507\u001b[0m ):\n\u001b[1;32m    508\u001b[0m     init_lora_weights \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_lora_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 510\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m     LoraLayer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_features\u001b[38;5;241m=\u001b[39min_features, out_features\u001b[38;5;241m=\u001b[39mout_features)\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# Freezing the pre-trained weight matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "answers = get_model_answers(0, inputs['input_ids'][:3], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "928e26f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor([inputs['input_ids'][0]]).cuda()[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0876a8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 15:02:46,257\tWARNING worker.py:2019 -- Warning: The remote function __main__.get_model_answers is very large (15 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m ===================================BUG REPORT===================================\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m Welcome to bitsandbytes. For bug reports, please run\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m python -m bitsandbytes\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m  and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m ================================================================================\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m bin /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m CUDA SETUP: Detected CUDA version 117\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m CUDA SETUP: Loading binary /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/laal_intern003/anaconda/envs/legal-master did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m   warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m   warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/10100/vscode-ipc-59cf26d7-3f13-45b8-ae2c-a7bd3be74a22.sock')}\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m   warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m Either way, this might cause trouble in the future:\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "\u001b[2m\u001b[36m(pid=1603374)\u001b[0m   warn(msg)\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.91s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.69s/it]\n"
     ]
    },
    {
     "ename": "RayTaskError(ValueError)",
     "evalue": "\u001b[36mray::get_model_answers()\u001b[39m (pid=1603374, ip=192.168.0.104)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_1530658/2231384656.py\", line 27, in get_model_answers\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/generation/utils.py\", line 1267, in generate\n    self._validate_model_kwargs(model_kwargs.copy())\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/generation/utils.py\", line 1140, in _validate_model_kwargs\n    raise ValueError(\nValueError: The following `model_kwargs` are not used by the model: ['max_gen_len'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(ValueError)\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m past_datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m----> 3\u001b[0m answers \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_model_answers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m current_datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      5\u001b[0m time \u001b[38;5;241m=\u001b[39m current_datetime \u001b[38;5;241m-\u001b[39m past_datetime\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/ray/_private/auto_init_hook.py:18\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     17\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/ray/_private/worker.py:2540\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2538\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(ValueError)\u001b[0m: \u001b[36mray::get_model_answers()\u001b[39m (pid=1603374, ip=192.168.0.104)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_1530658/2231384656.py\", line 27, in get_model_answers\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/generation/utils.py\", line 1267, in generate\n    self._validate_model_kwargs(model_kwargs.copy())\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/generation/utils.py\", line 1140, in _validate_model_kwargs\n    raise ValueError(\nValueError: The following `model_kwargs` are not used by the model: ['max_gen_len'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "past_datetime = datetime.datetime.now()\n",
    "answers = ray.get(get_model_answers.remote(0, inputs['input_ids'], tokenizer))\n",
    "current_datetime = datetime.datetime.now()\n",
    "time = current_datetime - past_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a441de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shortuuid\n",
    "from peft import PeftModel\n",
    "\n",
    "# define model\n",
    "adapter_path = './checkpoints_unmasked'\n",
    "\n",
    "base_path = './llama'\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    base_path,\n",
    "    device_map = 'auto',\n",
    "    low_cpu_mem_usage = True,\n",
    "    torch_dtype = torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    torch_dtype = torch.float16,\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "    \n",
    "@ray.remote(num_gpus = 2)\n",
    "@torch.inference_mode()\n",
    "def get_model_answers_2(model, input_ids, tokenizer): # questions: {'questions': input} \n",
    "    # get answers\n",
    "    output_ids = model.generate(\n",
    "        torch.as_tensor(input_ids).cuda(),\n",
    "        max_gen_len = 0.8,\n",
    "        temperature = 0.95, # float\n",
    "    )[0][len(inputs.input_ids):] # only take the answer, not the question itself\n",
    "\n",
    "    answers = tokenizer.decode(output_ids)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53c534b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 22172, 2]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['hello']).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f07c3683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def run_eval(model_id, dataset, answer_path, num_gpus):\n",
    "    dataset = make_dataset('./dataset')\n",
    "    question = dataset.select_columns(['question', 'idx'])\n",
    "    \n",
    "    chunk_size = len(question) // num_gpus\n",
    "    ans_handles = []\n",
    "    \n",
    "    for i in range(0, len(question), chunk_size):\n",
    "        ans_handles.append(\n",
    "        get_model_answers.remote(\n",
    "            model_id, question[i : i + chunk_size]\n",
    "            )\n",
    "        )\n",
    "    answers = []\n",
    "    \n",
    "    for ans_handle in ans_handles:\n",
    "        answers.extend(ray.get(ans_handle))\n",
    "        \n",
    "    with open('./dataset/answers', 'wb') as f:\n",
    "        pickle.dump(answers, f)\n",
    "\n",
    "@ray.remote(num_gpus = 2)\n",
    "@torch.inference_mode()\n",
    "def get_model_answers(model_id, questions):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        './llama',\n",
    "        add_eos_token = True # add end-of-sentence token\n",
    "    )\n",
    "    tokenizer.pad_token_id = 0\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        './llama',\n",
    "        device_map = 'auto').cuda()\n",
    "    \n",
    "    answers = []\n",
    "    for i, question in enumerate(tqdm(questions)):\n",
    "        prompt = question['question']\n",
    "        input_ids = tokenizer([prompt]).input_ids # [[]]\n",
    "        output_ids = model.generate(\n",
    "            torch.as_tensor(input_ids).cuda(),\n",
    "            temperature = 0.7,\n",
    "            max_new_tokens = 1024,\n",
    "        )\n",
    "        output_ids = output_ids[0][len(input_ids[0]):] # only take the answer\n",
    "        outputs = tokenizer.decode(output_ids, skip_special_tokens = True).strip()\n",
    "        \n",
    "        ans_id = shortuuid.uuid()\n",
    "        answers.append(\n",
    "            {\n",
    "                \"question_id\": question['idx'],\n",
    "                \"answer\": outputs,\n",
    "                \"answer_id\": ans_id,\n",
    "                \"model_id\": model_id,\n",
    "            })\n",
    "        \n",
    "        return answers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a13e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data_dir):\n",
    "    # build dataset\n",
    "    path = data_dir\n",
    "\n",
    "    # load file\n",
    "    if os.path.isfile(os.path.join(path, 'prompt.pkl')):\n",
    "        with open(os.path.join(path, 'prompt.pkl'), 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "    else:\n",
    "        dataset_list = build_dataset(['case_hold'], path)\n",
    "        dataset = dataset_list[0]['test']\n",
    "        random.seed(7)\n",
    "        dataset = dataset.shuffle().map(prompt_engineering).select_columns(['question', 'label'])\n",
    "        dataset = dataset.add_column(name = 'idx', column = range(3600))\n",
    "        # save file\n",
    "        with open(os.path.join(path, 'prompt.pkl'), 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def prompt_engineering(data_point):\n",
    "\n",
    "    prompt_cands = [\n",
    "    \"Please select the most suitable summary of the legal ruling that accompanies the relevant referenced decisions for the specific case. The following excerpt is from the court's decision.\",\n",
    "    \"Kindly choose the concise summary of the legal ruling that accompanies the relevant referenced decisions applicable to the given case. Provided below is an excerpt from the court decision.\",\n",
    "    \"Please decide on the most appropriate summary of the legal ruling that accompanies the relevant referenced decisions, which are relevant to the given case. Here is an excerpt from the court decision for your consideration.\",\n",
    "    \"Here is an excerpt from the court decision for the case. Please choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Consider the following excerpt from the court decision for the case. Your task is to select the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Given the excerpt from the court decision for the case, your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Please refer to the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case, using the excerpt from the court decision provided below.\",\n",
    "    \"Please review the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Here is the excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\"\n",
    "    ] # generated by Chat-GPT\n",
    "\n",
    "    \n",
    "    _slice = data_point['context'].find('(<HOLDING>)')\n",
    "    excerpt = data_point['context'][:_slice]\n",
    "    choices = [str(n) + f': {c}' for n,c in enumerate(data_point['endings'])]\n",
    "    prompt = random.choice(prompt_cands)\n",
    "    input = prompt + f'\\nExcerpt: {excerpt}' + f'\\nChoices: {choices}'\n",
    "\n",
    "    return {\n",
    "        'question' : input,\n",
    "        'label' : data_point['label']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9442b877",
   "metadata": {},
   "outputs": [
    {
     "ename": "RayTaskError(RuntimeError)",
     "evalue": "\u001b[36mray::get_model_answers()\u001b[39m (pid=1608809, ip=192.168.0.104)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_1530658/3220729133.py\", line 32, in get_model_answers\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2704, in from_pretrained\n    max_memory = get_balanced_memory(\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/accelerate/utils/modeling.py\", line 483, in get_balanced_memory\n    max_memory = get_max_memory(max_memory)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/accelerate/utils/modeling.py\", line 391, in get_max_memory\n    _ = torch.tensor([0], device=i)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./dataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[98], line 18\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m(model_id, dataset, answer_path, num_gpus)\u001b[0m\n\u001b[1;32m     15\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ans_handle \u001b[38;5;129;01min\u001b[39;00m ans_handles:\n\u001b[0;32m---> 18\u001b[0m     answers\u001b[38;5;241m.\u001b[39mextend(\u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mans_handle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/answers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     21\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(answers, f)\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/ray/_private/auto_init_hook.py:18\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     17\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/ray/_private/worker.py:2540\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2538\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m: \u001b[36mray::get_model_answers()\u001b[39m (pid=1608809, ip=192.168.0.104)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_1530658/3220729133.py\", line 32, in get_model_answers\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2704, in from_pretrained\n    max_memory = get_balanced_memory(\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/accelerate/utils/modeling.py\", line 483, in get_balanced_memory\n    max_memory = get_max_memory(max_memory)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/accelerate/utils/modeling.py\", line 391, in get_max_memory\n    _ = torch.tensor([0], device=i)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 16:03:00,694\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::get_model_answers()\u001b[39m (pid=1608918, ip=192.168.0.104)\n",
      "  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1530658/3220729133.py\", line 32, in get_model_answers\n",
      "  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2704, in from_pretrained\n",
      "    max_memory = get_balanced_memory(\n",
      "  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/accelerate/utils/modeling.py\", line 483, in get_balanced_memory\n",
      "    max_memory = get_max_memory(max_memory)\n",
      "  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/accelerate/utils/modeling.py\", line 391, in get_max_memory\n",
      "    _ = torch.tensor([0], device=i)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "run_eval(0, dataset, './dataset', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42707109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m   warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m \n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m ===================================BUG REPORT===================================\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m Welcome to bitsandbytes. For bug reports, please run\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m \n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m python -m bitsandbytes\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m \n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m  and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m ================================================================================\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m bin /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626970)\u001b[0m CUDA SETUP: Loading binary /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(get_model_answers pid=3626970)\u001b[0m Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(get_model_answers pid=3626970)\u001b[0m ****{'': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(calculate_metric pid=3626870)\u001b[0m \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626870)\u001b[0m ****{'': 0}\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626870)\u001b[0m ****{'': 0}\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626870)\u001b[0m ****{'': 0}\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626870)\u001b[0m ****{'': 0}\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626870)\u001b[0m ****{'': 0}\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626870)\u001b[0m ****{'': 0}\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626870)\u001b[0m ****{'': 0}\n",
      "\u001b[2m\u001b[36m(calculate_metric pid=3626870)\u001b[0m ****{'': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(get_model_answers pid=3626870)\u001b[0m \r",
      "Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(get_model_answers pid=3626870)\u001b[0m \r",
      "Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "ename": "RayTaskError(RuntimeError)",
     "evalue": "\u001b[36mray::get_model_answers()\u001b[39m (pid=3626870, ip=192.168.0.107)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_3622728/93050764.py\", line 110, in get_model_answers\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2777, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3118, in _load_pretrained_model\n    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 710, in _load_state_dict_into_meta_model\n    set_module_8bit_tensor_to_device(\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/utils/bitsandbytes.py\", line 87, in set_module_8bit_tensor_to_device\n    new_value = value.to(device)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 247, in _lazy_init\n    torch._C._cuda_init()\nRuntimeError: No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 233\u001b[0m\n\u001b[1;32m    227\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args(args \u001b[38;5;241m=\u001b[39m [])\n\u001b[1;32m    229\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m -   \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    230\u001b[0m                     datefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    231\u001b[0m                     level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mDEBUG)\n\u001b[0;32m--> 233\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 157\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model_id, data_dir, answer_path, num_gpus)\u001b[0m\n\u001b[1;32m    154\u001b[0m model_name \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama_legal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama_chat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama_legal_chat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama_chat_legal\u001b[39m\u001b[38;5;124m'\u001b[39m][model_id]\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# get answers\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(answer_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./answers.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    160\u001b[0m     answers \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\u001b[38;5;241m.\u001b[39mselect_columns([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[3], line 91\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m(model_id, data_dir, answer_path, num_gpus)\u001b[0m\n\u001b[1;32m     88\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ans_handler \u001b[38;5;129;01min\u001b[39;00m ans_handlers:\n\u001b[0;32m---> 91\u001b[0m     answers\u001b[38;5;241m.\u001b[39mextend(\u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mans_handler\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     92\u001b[0m answers \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_pandas(pd\u001b[38;5;241m.\u001b[39mDataFrame(answers)) \u001b[38;5;66;03m# convert to Dataset obj\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# save the answer\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/ray/_private/auto_init_hook.py:18\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     17\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/ray/_private/worker.py:2540\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2538\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m: \u001b[36mray::get_model_answers()\u001b[39m (pid=3626870, ip=192.168.0.107)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_3622728/93050764.py\", line 110, in get_model_answers\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2777, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3118, in _load_pretrained_model\n    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 710, in _load_state_dict_into_meta_model\n    set_module_8bit_tensor_to_device(\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/utils/bitsandbytes.py\", line 87, in set_module_8bit_tensor_to_device\n    new_value = value.to(device)\n  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 247, in _lazy_init\n    torch._C._cuda_init()\nRuntimeError: No CUDA GPUs are available"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 15:06:23,622\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::get_model_answers()\u001b[39m (pid=3626970, ip=192.168.0.107)\n",
      "  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3622728/93050764.py\", line 110, in get_model_answers\n",
      "  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2777, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3118, in _load_pretrained_model\n",
      "    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n",
      "  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 710, in _load_state_dict_into_meta_model\n",
      "    set_module_8bit_tensor_to_device(\n",
      "  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/utils/bitsandbytes.py\", line 87, in set_module_8bit_tensor_to_device\n",
      "    new_value = value.to(device)\n",
      "  File \"/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 247, in _lazy_init\n",
      "    torch._C._cuda_init()\n",
      "RuntimeError: No CUDA GPUs are available\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import pickle\n",
    "import ray\n",
    "import tqdm\n",
    "import shortuuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "## dataset\n",
    "from utils.dataset import *\n",
    "\n",
    "## models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# pipeline\n",
    "def make_dataset(data_dir):\n",
    "    # build dataset\n",
    "    path = data_dir\n",
    "\n",
    "    # load file\n",
    "    if os.path.isfile(os.path.join(path, 'prompt.pkl')):\n",
    "        with open(os.path.join(path, 'prompt.pkl'), 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "    else:\n",
    "        dataset_list = build_dataset(['case_hold'], path)\n",
    "        dataset = dataset_list[0]['test']\n",
    "        random.seed(7)\n",
    "        dataset = dataset.shuffle().map(prompt_engineering).select_columns(['question', 'label'])\n",
    "        dataset = dataset.add_column(name = 'idx', column = range(3600))\n",
    "        # save file\n",
    "        with open(os.path.join(path, 'prompt.pkl'), 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def prompt_engineering(data_point):\n",
    "\n",
    "    prompt_cands = [\n",
    "    \"Please select the most suitable summary of the legal ruling that accompanies the relevant referenced decisions for the specific case. The following excerpt is from the court's decision.\",\n",
    "    \"Kindly choose the concise summary of the legal ruling that accompanies the relevant referenced decisions applicable to the given case. Provided below is an excerpt from the court decision.\",\n",
    "    \"Please decide on the most appropriate summary of the legal ruling that accompanies the relevant referenced decisions, which are relevant to the given case. Here is an excerpt from the court decision for your consideration.\",\n",
    "    \"Here is an excerpt from the court decision for the case. Please choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Consider the following excerpt from the court decision for the case. Your task is to select the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Given the excerpt from the court decision for the case, your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Please refer to the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case, using the excerpt from the court decision provided below.\",\n",
    "    \"Please review the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Here is the excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\"\n",
    "    ] # generated by Chat-GPT\n",
    "\n",
    "    \n",
    "    _slice = data_point['context'].find('(<HOLDING>)')\n",
    "    excerpt = data_point['context'][:_slice]\n",
    "    choices = [str(n) + f': {c}' for n,c in enumerate(data_point['endings'])]\n",
    "    prompt = random.choice(prompt_cands)\n",
    "    input = prompt + f'\\nExcerpt: {excerpt}' + f'\\nChoices: {choices}'\n",
    "\n",
    "    return {\n",
    "        'question' : input,\n",
    "        'label' : data_point['label']\n",
    "    }\n",
    "\n",
    "def run_eval(model_id, data_dir, answer_path, num_gpus = 3):\n",
    "    dataset = make_dataset(data_dir)\n",
    "    questions = dataset.select_columns(['question', 'idx'])\n",
    "\n",
    "    chunk_size = len(questions) // num_gpus\n",
    "    ans_handlers = []\n",
    "\n",
    "    for i in range(0, len(questions), chunk_size):\n",
    "        ans_handlers.append(\n",
    "            get_model_answers.remote(\n",
    "                model_id, questions[i:i+chunk_size]\n",
    "            )\n",
    "        )\n",
    "    answers = []\n",
    "\n",
    "    for ans_handler in ans_handlers:\n",
    "        answers.extend(ray.get(ans_handler))\n",
    "    answers = datasets.Dataset.from_pandas(pd.DataFrame(answers)) # convert to Dataset obj\n",
    "\n",
    "    # save the answer\n",
    "    with open(os.path.join(answer_path, 'answers.pkl'), 'wb') as f:\n",
    "        pickle.dump(answers, f)\n",
    "\n",
    "@ray.remote\n",
    "@torch.inference_mode()\n",
    "def get_model_answers(model_id, questions):\n",
    "    \n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        './llama',\n",
    "        add_eos_token = True\n",
    "    )\n",
    "    tokenizer.pad_token_id = 0 # we will pad the sequence til the max length\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    print(f'****{device_map}')\n",
    "\n",
    "    base = LlamaForCausalLM.from_pretrained(\n",
    "        './llama',\n",
    "        device_map = device_map,\n",
    "        load_in_8bit = True,\n",
    "    )\n",
    "    #adapter_path = './adapter/'+['llama_legal', 'llama_chat', 'llama_legal_chat', 'llama_chat_legal'][model_id]\n",
    "    adapter_path = './LegalAdapterTrainig/checkpoints_unmasked'\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base,\n",
    "        adapter_path,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    print(\"Applying the LoRA\")\n",
    "    model = lora_model.merge_and_unload().cuda()\n",
    "\n",
    "\n",
    "    answers = []\n",
    "\n",
    "    for i, question in enumerate(tqdm(questions)):\n",
    "        prompt = question['question']\n",
    "        input_ids = tokenizer([prompt]).input_ids # [[]]\n",
    "        output_ids = model.generate(\n",
    "            torch.as_tensor(input_ids).cuda(),\n",
    "            temperature = 0.7, # manipulates how strict the model will follow the prompt instruction\n",
    "            max_new_tokens = 1024,\n",
    "        )\n",
    "        output_ids = output_ids[0][len(input_ids[0]) : ] # only take the answer, not the question prompt from the outputs\n",
    "        outputs = tokenizer.decode(output_ids, skip_special_tokens = True).strip()\n",
    "        ans_id = shortuuid.uuid()\n",
    "        answers.append(\n",
    "            {\n",
    "                \"idx\" : question['idx'],\n",
    "                \"answer\" : outputs,\n",
    "                \"answer_id\" : ans_id,\n",
    "                \"model_id\" : model_id\n",
    "\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return answers\n",
    "\n",
    "\n",
    "def evaluate(model_id, data_dir, answer_path, num_gpus):\n",
    "    # define model\n",
    "    model_name = ['llama_legal', 'llama_chat', 'llama_legal_chat', 'llama_chat_legal'][model_id]\n",
    "\n",
    "    # get answers\n",
    "    run_eval(model_id, data_dir, answer_path, num_gpus)\n",
    "\n",
    "    with open(os.path.join(answer_path, './answers.pkl', 'rb')) as f:\n",
    "        answers = pickle.load(f).select_columns(['answer', 'idx'])\n",
    "    labels = dataset.select_columns(['label', 'idx'])\n",
    "\n",
    "    dataset = datasets.concatenate_datasets([answers, labels])\n",
    "\n",
    "    # calculate metric\n",
    "    calmet_handlers = []\n",
    "    chunk_size = len(dataset) // num_gpus\n",
    "\n",
    "    ray.shutdown()\n",
    "    ray.init(num_gpus = num_gpus)\n",
    "\n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        calmet_handlers.append(\n",
    "            calculate_metric.remote(dataset[i:i+chunk_size])\n",
    "            )\n",
    "    \n",
    "    results = [] \n",
    "    \n",
    "    for calmet_handler in calmet_handlers:\n",
    "        results.append(ray.get(calmet_handler)) # e.g. [[0,0,0,0,0,0,0],[0,0,0,0,0,0,0], ...]\n",
    "    \n",
    "    metrics = results.sum(axis = 0)\n",
    "    accuracy = metrics[1] / (results[0] + results[1])\n",
    "    \n",
    "    print(f'The accuracy of the {model_name} is : {accuracy}')\n",
    "\n",
    "@ray.remote\n",
    "def calculate_metric(dataset):\n",
    "\n",
    "    for i, data in enumerate(dataset):\n",
    "\n",
    "        idx = dataset['idx']\n",
    "        answer = dataset['answer']\n",
    "        label = dataset['label']\n",
    "\n",
    "        incorrect_correct_labels = np.array([0,0,0,0,0,0,0]) # one-hot-encoding: [incorrect, correct, 0,1,2,3,4 (ground truth)]\n",
    "\n",
    "        r = int(label in answer) # incorrect: 0, correct: 1\n",
    "        incorrect_correct_labels[r] += 1\n",
    "        incorrect_correct_labels[label+2] += 1\n",
    "    \n",
    "    return incorrect_correct_labels\n",
    "\n",
    "\n",
    "#def chatgpt_get_answer(gpt_key):\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir',\n",
    "                        type = str,\n",
    "                        default = './data/prompt',\n",
    "                        help = 'Where LexGlue dataset is stored')\n",
    "    parser.add_argument('--answer_dir',\n",
    "                        type = str,\n",
    "                        default = './data/answers',\n",
    "                        help = 'Where answers from the model is stored')\n",
    "    parser.add_argument('--gpu_num',\n",
    "                        type = int,\n",
    "                        default = 4,\n",
    "                        help = 'The number of gpus to use for evaluation')\n",
    "    parser.add_argument('--model_id',\n",
    "                        type = int,\n",
    "                        default = 0, \n",
    "                        help = '0: llama_legal, 1: llama_chat, 2: llama_legal_chat, 3: llama_chat_legal')\n",
    "\n",
    "    args = parser.parse_args(args = [])\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                        level=logging.DEBUG)\n",
    "\n",
    "    evaluate(args.model_id, args.data_dir, args.answer_dir, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "428410ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7345ec8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d592c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.39.1-py3-none-any.whl (97.1 MB)\n",
      "\u001b[K     |███████████▏                    | 33.9 MB 3.6 MB/s eta 0:00:18"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5863ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are succesfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]Loading cached processed dataset at /home/laal_intern003/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a/cache-9b631132480454cd.arrow\n",
      "Loading cached processed dataset at /home/laal_intern003/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a/cache-819a6e663756257e.arrow\n",
      "Loading cached processed dataset at /home/laal_intern003/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a/cache-c54604b7b969a144.arrow\n",
      "100%|██████████| 3/3 [00:00<00:00, 236.29it/s]\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4005a6e9f65148d481e716aa7800dc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:17.891M, rate 0.26%\n"
     ]
    }
   ],
   "source": [
    "# import modules: utils\n",
    "from calendar import EPOCH\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import bitsandbytes as bn\n",
    "import argparse\n",
    "\n",
    "# import modules: learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import (\n",
    "    prepare_model_for_int8_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "# import modules: dataset\n",
    "from datasets import load_dataset\n",
    "from utils.dataset import *\n",
    "\n",
    "MODEL = 0\n",
    "\n",
    "def main(args):\n",
    "    # parameters\n",
    "    MICRO_BATCH_SIZE = args.micro_batch_size\n",
    "    BATCH_SIZE = 64\n",
    "    GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "    model_size = args.model_size # The size of Llama (i.e.,7B, 13B, 30B)\n",
    "    EPOCHS = 1\n",
    "    LEARNING_RATE = args.learning_rate\n",
    "    CUTOFF_LEN = 512\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT = 0.05\n",
    "    # VAL_SET_SIZE = 2000 -- we don't need this cuz LexGLUE is already splited into train/val/train\n",
    "    TARGET_MODULES = [\n",
    "        \"q_proj\", # query projection\n",
    "        \"k_proj\", # key projection\n",
    "        \"v_proj\", # value projection\n",
    "        \"down_proj\", # down projection\n",
    "        \"gate_proj\", # gate projection\n",
    "        \"up_proj\"   # up projection\n",
    "    ]\n",
    "\n",
    "    # load data\n",
    "    task_list = args.task_list[0]\n",
    "    path = args.data_dir\n",
    "    dataset_list = build_dataset(task_list, path)\n",
    "    dataset = dataset_list[0]\n",
    "    dataset = rebuild_dataset(dataset)\n",
    "    print(\"Datasets are loaded\") # dict, {task_name:data}\n",
    "\n",
    "    # load model\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\",1))\n",
    "    ddp = world_size != 1 # if there are multiple GPUs, we will do distributed learning\n",
    "    \n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} # process id for the corresponding gpu\n",
    "        GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n",
    "    \n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        'llama',\n",
    "        load_in_8bit = True, # this saves memory\n",
    "        device_map = device_map)\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        'llama',\n",
    "        add_eos_token = True # add end-of-sentence token\n",
    "    )\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r = LORA_R, # determines the compression rate\n",
    "        lora_alpha = LORA_ALPHA,\n",
    "        target_modules = TARGET_MODULES,\n",
    "        lora_dropout = LORA_DROPOUT,\n",
    "        bias = \"none\",\n",
    "        task_type = \"CAUSAL_LM\"\n",
    "        )\n",
    "\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    config.save_pretrained(args.output_dir)\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    tokenizer.pad_token_id = 0\n",
    "\n",
    "    # count the total nums of params in the model\n",
    "    total_params, params = 0, 0 \n",
    "    for n, p in model.model.named_parameters():\n",
    "        if any([x in n for x in [\"lora\"]]):\n",
    "            total_params += p.numel()\n",
    "        params += p.numel()\n",
    "    print(\n",
    "        f'Total number of parameters:{total_params // 1000 / 1000}M, rate {round(total_params / params * 100, 2)}%'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_size',\n",
    "                        type = str,\n",
    "                        default = '7B',\n",
    "                        help = 'Foundation model size (i.e., 7B, 13B, 30B)')\n",
    "    parser.add_argument('--micro_batch_size',\n",
    "                        type = int,\n",
    "                        default = 16,\n",
    "                        help = 'Batch size') # The bigger, the faster\n",
    "    parser.add_argument('--learning_rate',\n",
    "                        type = float,\n",
    "                        default = 0.0002,\n",
    "                        help = 'Learning rate')\n",
    "    parser.add_argument('--task_list',\n",
    "                        default = ['case_hold'],\n",
    "                        help = 'The list of datasets you want to download')\n",
    "    parser.add_argument('--data_dir',\n",
    "                        type = str,\n",
    "                        default = 'data',\n",
    "                        help = 'Where LexGlue dataset is stored')\n",
    "    parser.add_argument('--output_dir',\n",
    "                        type = str,\n",
    "                        default = './checkpoints_16',\n",
    "                        help = 'Where the adapter is stored')\n",
    "    \n",
    "    args = parser.parse_args(args = [])\n",
    "    args.task_list = [['case_hold']]\n",
    "    model = main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65338452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0058, -0.0076, -0.0059,  ..., -0.0132,  0.0022,  0.0089],\n",
       "         [-0.0006, -0.0140,  0.0006,  ..., -0.0051,  0.0045, -0.0001],\n",
       "         [-0.0077, -0.0127,  0.0097,  ..., -0.0092,  0.0074, -0.0142],\n",
       "         ...,\n",
       "         [-0.0060,  0.0086, -0.0096,  ..., -0.0043, -0.0104, -0.0108],\n",
       "         [ 0.0061,  0.0106,  0.0141,  ..., -0.0079, -0.0135,  0.0105],\n",
       "         [ 0.0038,  0.0043, -0.0086,  ...,  0.0111,  0.0105,  0.0062]],\n",
       "        device='cuda:1')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.data for t in model.base_model.model.model.layers[26].self_attn.q_proj.lora_A.parameters()] # initialized loraB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0290eb64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mtensor\u001b[49m([[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0144\u001b[39m,  \u001b[38;5;241m0.0114\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0118\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,  \u001b[38;5;241m0.0023\u001b[39m,  \u001b[38;5;241m0.0082\u001b[39m,  \u001b[38;5;241m0.0005\u001b[39m],\n\u001b[1;32m      2\u001b[0m         [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0023\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0016\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0012\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,  \u001b[38;5;241m0.0063\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0027\u001b[39m,  \u001b[38;5;241m0.0002\u001b[39m],\n\u001b[1;32m      3\u001b[0m         [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0069\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0153\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0133\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0156\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0039\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0030\u001b[39m],\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\n\u001b[1;32m      5\u001b[0m         [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0130\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0028\u001b[39m,  \u001b[38;5;241m0.0066\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0103\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0144\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0106\u001b[39m],\n\u001b[1;32m      6\u001b[0m         [ \u001b[38;5;241m0.0103\u001b[39m,  \u001b[38;5;241m0.0028\u001b[39m,  \u001b[38;5;241m0.0138\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,  \u001b[38;5;241m0.0114\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0110\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0117\u001b[39m],\n\u001b[1;32m      7\u001b[0m         [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0089\u001b[39m,  \u001b[38;5;241m0.0137\u001b[39m,  \u001b[38;5;241m0.0061\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,  \u001b[38;5;241m0.0134\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0156\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0115\u001b[39m]],\n\u001b[1;32m      8\u001b[0m        dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": [
    "('base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', \n",
    " tensor([[-0.0144,  0.0114, -0.0118,  ...,  0.0023,  0.0082,  0.0005],\n",
    "        [-0.0023, -0.0016, -0.0012,  ...,  0.0063, -0.0027,  0.0002],\n",
    "        [-0.0069, -0.0153, -0.0133,  ..., -0.0156, -0.0039, -0.0030],\n",
    "        ...,\n",
    "        [-0.0130, -0.0028,  0.0066,  ..., -0.0103, -0.0144, -0.0106],\n",
    "        [ 0.0103,  0.0028,  0.0138,  ...,  0.0114, -0.0110, -0.0117],\n",
    "        [-0.0089,  0.0137,  0.0061,  ...,  0.0134, -0.0156, -0.0115]],\n",
    "       dtype=torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "201d6cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/laal_intern003/.cache/huggingface/datasets/json/default-115c9468d204d314/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1555493d71d4e9aa1846d70299bfbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4427c9d1a76449c48ec98b7e5eb4ad02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06c1c6187a543778269deb3176315fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/laal_intern003/.cache/huggingface/datasets/json/default-115c9468d204d314/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df986b8574ae4bf9a8282660aa0eb8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "decapoda-research/llama-6B-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:259\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/decapoda-research/llama-6B-hf/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/huggingface_hub/file_download.py:1541\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1532\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[0;32m-> 1541\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:291\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    283\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m     )\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6492815d-0a943bff7d0aff3531a5ec01)\n\nRepository Not Found for url: https://huggingface.co/decapoda-research/llama-6B-hf/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOCAL_RANK\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m)}\n\u001b[1;32m     57\u001b[0m     GRADIENT_ACCUMULATION_STEPS \u001b[38;5;241m=\u001b[39m GRADIENT_ACCUMULATION_STEPS \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m world_size\n\u001b[0;32m---> 59\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecapoda-research/llama-\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m total_params, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     66\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecapoda-research/llama-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(size), add_eos_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/modeling_utils.py:2251\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2250\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 2251\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2264\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2267\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/configuration_utils.py:547\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrainedConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    471\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 547\u001b[0m     config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[1;32m    549\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    550\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    552\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/configuration_utils.py:574\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 574\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    576\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/configuration_utils.py:629\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/envs/legal-master/lib/python3.8/site-packages/transformers/utils/hub.py:433\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: decapoda-research/llama-6B-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import (\n",
    "    prepare_model_for_int8_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "\n",
    "# Parameters\n",
    "MICRO_BATCH_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "size = \"6B\"\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.0002\n",
    "CUTOFF_LEN = 512\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "VAL_SET_SIZE = 2000\n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "]\n",
    "DATA_PATH = \"data/data_tmp.json\"\n",
    "OUTPUT_DIR = \"checkpoints/{}\".format(size)\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "# Load data\n",
    "data = []\n",
    "random.shuffle(data)\n",
    "json.dump(data, open(DATA_PATH, \"w\"))\n",
    "data = load_dataset(\"json\", data_files=DATA_PATH)\n",
    "\n",
    "# Load Model\n",
    "device_map = \"auto\"\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"decapoda-research/llama-{}-hf\".format(size),\n",
    "    load_in_8bit=True,\n",
    "    device_map=device_map,\n",
    ")\n",
    "total_params, params = 0, 0\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"decapoda-research/llama-{}-hf\".format(size), add_eos_token=True\n",
    ")\n",
    "\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "config.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "for n, p in model.model.named_parameters():\n",
    "    if any([x in n for x in [\"lora\"]]):\n",
    "        total_params += p.numel()\n",
    "    params += p.numel()\n",
    "\n",
    "print(\n",
    "    \"Total number of parameters: {}M, rate: {}%\".format(\n",
    "        total_params // 1000 / 1000, round(total_params / params * 100, 2)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95eec7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.data for t in model.base_model.model.model.layers[26].self_attn.q_proj.lora_B.parameters()] # initialized loraB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d589a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/laal_intern003/anaconda/envs/legal-master did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/10100/vscode-ipc-b71cb56e-ff7d-41c0-9383-cb7ffbe2f918.sock')}\n",
      "  warn(msg)\n",
      "/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/laal_intern003/anaconda/envs/legal-master/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61419a09de7a42308167a906f26e1ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ca498628fe4ff0afaad90ae72f9400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Dataset({\n",
      "    features: ['question', 'idx'],\n",
      "    num_rows: 3600\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: {'question': \"Given the excerpt from the court decision for the case, your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\\nExcerpt: W. Reginald Rose, Jr., and Laura J. Rose appeal pro se from the district court’s order dismissing their civil rights complaint for failing to comply with Rule 8 of the Federal Rules of Civil Procedure. We dismiss the appeal for lack of jurisdiction. The district court dismissed the Roses’ complaint without prejudice and granted leave to amend. Rather than filing an amended complaint or obtaining a final order of dismissal from the district court, the Roses filed a notice of appeal. We therefore lack jurisdiction. See WMX Techs., Inc. v. Miller, 104 F.3d 1133, 1136-37 (9th Cir.1997) (en banc) \\nChoices: ['0: holding that denial of a postconviction motion without prejudice and with leave to amend is not a final appealable order', '1: holding that dismissal with leave to amend is not a final order', '2: holding that a district courts dismissal that expressly grants leave to amend is not final and that a final judgment must be obtained before such a case becomes appealable', '3: holding that a district court judgment is not a final judgment appealable by the defendant unless it includes the final adjudication and the final sentence', '4: holding that when a district court expressly grants leave to amend it is plain that the order is not final']\", 'idx': 0}\n",
      "length of the input_ids: 369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3600 [00:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['\\nThe Roses’ complaint alleges that the district court violated their due process rights by denying them a fair trial and by failing to provide them with a speedy trial. The district court dismissed the complaint without prejudice and granted the Roses leave to amend. The Roses did not file an amended complaint or obtain a final order of dismissal from the district court. Instead, they filed a notice of appeal. We therefore lack jurisdiction. See WMX Techs., Inc. v. Miller, 104 F.3d 1133, 1136-37 (9th Cir.1997) (en banc).\\nThe Roses’ complaint alleges that the district court violated their due process rights by denying them a fair trial and by failing to provide them with a speedy trial. The district court dismissed the complaint without prejudice and granted the Roses leave to amend. The Roses did not file an amended complaint or obtain a final order of dismissal from the district court. Instead, they filed a notice of appeal. We therefore lack jurisdiction. See WMX Techs., Inc. v. Miller, 104 F.3d 1133, 1136-37 (9th Cir.1997) (en banc).\\nThe Roses’ complaint alleges that the district court violated their due process rights by denying them a fair trial and by failing to provide them with a speedy trial. The district court dismissed the complaint without prejudice and granted the Roses leave to amend. The Roses did not file an amended complaint or obtain a final order of dismissal from the district court. Instead, they filed a notice of appeal. We therefore lack jurisdiction. See WMX Techs., Inc. v. Miller, 104 F.3d 1133, 1136-37 (9th Cir.1997) (en banc).\\nThe Roses’ complaint alleges that the district court violated their due process rights by denying them a fair trial and by failing to provide them with a speedy trial. The district court dismissed the complaint without prejudice and granted the Roses leave to amend. The Roses did not']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import pickle\n",
    "import ray\n",
    "import tqdm\n",
    "import shortuuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import bitsandbytes as bn\n",
    "import tqdm\n",
    "\n",
    "## dataset\n",
    "from utils.dataset import *\n",
    "from utils.model import *\n",
    "\n",
    "## models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# pipeline\n",
    "def make_dataset(data_dir):\n",
    "    # build dataset\n",
    "    path = data_dir\n",
    "\n",
    "    # load file\n",
    "    if os.path.isfile(os.path.join(path, 'prompt.pkl')):\n",
    "        with open(os.path.join(path, 'prompt.pkl'), 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "    else:\n",
    "        dataset_list = build_dataset(['case_hold'], path)\n",
    "        dataset = dataset_list[0]['test']\n",
    "        random.seed(7)\n",
    "        dataset = dataset.shuffle().map(prompt_engineering).select_columns(['question', 'label'])\n",
    "        dataset = dataset.add_column(name = 'idx', column = range(3600))\n",
    "        # save file\n",
    "        with open(os.path.join(path, 'prompt.pkl'), 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def prompt_engineering(data_point):\n",
    "\n",
    "    prompt_cands = [\n",
    "    \"Please select the most suitable summary of the legal ruling that accompanies the relevant referenced decisions for the specific case. The following excerpt is from the court's decision.\",\n",
    "    \"Kindly choose the concise summary of the legal ruling that accompanies the relevant referenced decisions applicable to the given case. Provided below is an excerpt from the court decision.\",\n",
    "    \"Please decide on the most appropriate summary of the legal ruling that accompanies the relevant referenced decisions, which are relevant to the given case. Here is an excerpt from the court decision for your consideration.\",\n",
    "    \"Here is an excerpt from the court decision for the case. Please choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Consider the following excerpt from the court decision for the case. Your task is to select the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Given the excerpt from the court decision for the case, your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Please refer to the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case, using the excerpt from the court decision provided below.\",\n",
    "    \"Please review the following excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\",\n",
    "    \"Here is the excerpt from the court decision for the case. Your task is to choose the most appropriate short summary of the legal ruling that accompanies the referenced decisions relevant to the case.\"\n",
    "    ] # generated by Chat-GPT\n",
    "\n",
    "    \n",
    "    _slice = data_point['context'].find('(<HOLDING>)')\n",
    "    excerpt = data_point['context'][:_slice]\n",
    "    choices = [str(n) + f': {c}' for n,c in enumerate(data_point['endings'])]\n",
    "    prompt = random.choice(prompt_cands)\n",
    "    input = prompt + f'\\nExcerpt: {excerpt}' + f'\\nChoices: {choices}'\n",
    "\n",
    "    return {\n",
    "        'question' : input,\n",
    "        'label' : data_point['label']\n",
    "    }\n",
    "\n",
    "def run_eval(model_id, dataset, answer_path, num_gpus = 3):\n",
    "    questions = dataset.select_columns(['question', 'idx'])\n",
    "\n",
    "    # chunk_size = len(questions) // num_gpus\n",
    "    # ans_handlers = []\n",
    "\n",
    "    base_model_path = './llama'\n",
    "    #adapter_path = './adapter/'+['llama_legal', 'llama_chat', 'llama_legal_chat', 'llama_chat_legal'][model_id]\n",
    "    adapter_model_path = '/home/laal_intern003/LegalMaster/LegalAdapterTraining/checkpoints_unmasked'\n",
    "\n",
    "    tokenizer, _model, _device = load_tokenizer_and_model(base_model_path, adapter_model_path, load_8bit=True)\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n",
    " \n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained('llama', device_map = device_map, torch_dtype = torch.float16)\n",
    "\n",
    "    print(f'Device: {_device}')\n",
    "    answers = get_model_answers(tokenizer, model, questions, device_map)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for question in questions:\n",
    "    #         answers.append(get_model_answers(tokenizer, model, question))\n",
    "\n",
    "    answers = datasets.Dataset.from_pandas(pd.DataFrame(answers)) # convert to Dataset obj\n",
    "\n",
    "    if not os.path.exists(answer_path):\n",
    "        os.makedirs(answer_path)\n",
    "    # save the answer\n",
    "    with open(os.path.join(answer_path, f'answers_{model_id}.pkl'), 'wb') as f:\n",
    "        pickle.dump(answers, f)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def get_model_answers(tokenizer, model, questions, device_map):\n",
    "\n",
    "    answers = []\n",
    "#     print(questions)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _idx, question in enumerate(tqdm(questions)):\n",
    "            prompt = question['question']\n",
    "            input_ids = tokenizer([prompt], return_tensors = 'pt')['input_ids'][:, -1024:].cuda()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "#             print(f'question: {question}')\n",
    "\n",
    "            outputs = simple_decode(\n",
    "                input_ids,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                max_new_tokens = 512,\n",
    "            )\n",
    "\n",
    "            ans_id = shortuuid.uuid()\n",
    "            answers.append(\n",
    "                {\n",
    "                    \"idx\" : _idx,\n",
    "                    \"answer\" : outputs,\n",
    "                    \"answer_id\" : ans_id,\n",
    "                    # \"model_id\" : model_id\n",
    "\n",
    "                }\n",
    "                )\n",
    "            \n",
    "    return answers\n",
    "            \n",
    "\n",
    "def evaluate(model_id, data_dir, answer_path, num_gpus):\n",
    "    # define model\n",
    "\n",
    "    model_name = ['llama_legal', 'llama_chat', 'llama_legal_chat', 'llama_chat_legal'][model_id]\n",
    "\n",
    "    # get answers\n",
    "    dataset = make_dataset(data_dir)\n",
    "    run_eval(model_id, dataset, answer_path, num_gpus)\n",
    "\n",
    "    with open(os.path.join(answer_path, f'./answers_{model_id}.pkl'), 'rb') as f:\n",
    "        answers = pickle.load(f).select_columns(['answer', 'idx'])\n",
    "\n",
    "    labels = dataset.select_columns(['label', 'idx'])\n",
    "\n",
    "    dataset = datasets.concatenate_datasets([answers, labels])\n",
    "\n",
    "@ray.remote\n",
    "def calculate_metric(dataset):\n",
    "\n",
    "    for i, data in enumerate(dataset):\n",
    "\n",
    "        idx = dataset['idx']\n",
    "        answer = dataset['answer']\n",
    "        label = dataset['label']\n",
    "\n",
    "        incorrect_correct_labels = np.array([0,0,0,0,0,0,0]) # one-hot-encoding: [incorrect, correct, 0,1,2,3,4 (ground truth)]\n",
    "\n",
    "        r = int(label in answer) # incorrect: 0, correct: 1\n",
    "        incorrect_correct_labels[r] += 1\n",
    "        incorrect_correct_labels[label+2] += 1\n",
    "    \n",
    "    return incorrect_correct_labels\n",
    "\n",
    "\n",
    "#def chatgpt_get_answer(gpt_key):\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir',\n",
    "                        type = str,\n",
    "                        default = './data/prompt',\n",
    "                        help = 'Where LexGlue dataset is stored')\n",
    "    parser.add_argument('--answer_dir',\n",
    "                        type = str,\n",
    "                        default = './data/answers',\n",
    "                        help = 'Where answers from the model is stored')\n",
    "    parser.add_argument('--gpu_num',\n",
    "                        type = int,\n",
    "                        default = 2,\n",
    "                        help = 'The number of gpus to use for evaluation')\n",
    "    parser.add_argument('--model_id',\n",
    "                        type = int,\n",
    "                        default = 0, \n",
    "                        help = '0: llama_legal, 1: llama_chat, 2: llama_legal_chat, 3: llama_chat_legal')\n",
    "\n",
    "    args = parser.parse_args(args = [])\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                        level=logging.DEBUG)\n",
    "\n",
    "    evaluate(args.model_id, args.data_dir, args.answer_dir, args.gpu_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141aeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
