{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from datasets import load_dataset\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import argparse\n",
    "import socket\n",
    "from dataset import get_dataset, build_dataset, tokenize\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset lex_glue (/home/laal_intern003/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n",
      "100%|██████████| 3/3 [00:00<00:00, 375.51it/s]\n"
     ]
    }
   ],
   "source": [
    "path = './dataset'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "dataset = get_dataset('case_hold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['context', 'endings', 'label'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset['train']['context'])\n",
    "\n",
    "data = {}\n",
    "for split in ['train', 'test', 'validation']:\n",
    "    keys = dataset[f'{split}'].features.keys()\n",
    "    data[f'{split}'] = {}\n",
    "\n",
    "    for key in keys:\n",
    "        data[f'{split}'][f'{key}'] = dataset[f'{split}'][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset('case_hold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/laal_intern003/.cache/huggingface/datasets/json/default-423ee07a41f4a944/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2531.26it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 336.97it/s]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/laal_intern003/.cache/huggingface/datasets/json/default-423ee07a41f4a944/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 280.20it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '/home/laal_intern003/LegalMaster/datast/case_hold.json'\n",
    "data = load_dataset(\"json\", data_files=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset lex_glue (/home/laal_intern003/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n",
      "100%|██████████| 3/3 [00:00<00:00, 523.52it/s]\n"
     ]
    }
   ],
   "source": [
    "data_1 = load_dataset('lex_glue', 'case_hold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Found cached dataset lex_glue (/home/laal_intern003/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n",
      "100%|██████████| 3/3 [00:00<00:00, 491.67it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are succesfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(['case_hold'],'./dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'case_hold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcase_hold\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'case_hold' is not defined"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(path, f'{case_hold}.pkl')):\n",
    "    print('yes')\n",
    "    with open(os.path.join(path, f'{task}.pkl'), 'wb') as f:\n",
    "        pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './dataset'\n",
    "os.path.exists(os.path.join(path, 'case_hold.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/case_hold.pkl', 'rb') as f:\n",
    "    datast = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'endings', 'label'],\n",
       "        num_rows: 45000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['context', 'endings', 'label'],\n",
       "        num_rows: 3600\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['context', 'endings', 'label'],\n",
       "        num_rows: 3900\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(tokenize)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "dataset[0]['train'].shuffle()['context'].map(tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# data = load_dataset(\"json\", data_files=./data)\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    './llama',\n",
    "    add_eos_token = True # add end-of-sentence token\n",
    ")\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    return data_point[\"context\"]\n",
    "\n",
    "def tokenize(prompt,tokenizer, CUTOFF_LEN):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN + 1,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": result[\"input_ids\"][:-1],\n",
    "        \"attention_mask\": result[\"attention_mask\"][:-1],\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point, tokenizer, CUTOFF_LEN):\n",
    "    prompt = generate_prompt(data_point)\n",
    "    return tokenize(prompt, tokenizer, CUTOFF_LEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'topic': 'What is the step by step guide to invest in share market in india?',\n",
       "  'input': 'The conversation between human and AI assistant.\\n[|Human|] I want to know the step by step guide to invest in share market in India.\\n[|AI|] Sure, I can help with that. Firstly, you need to open a demat and trading account with a registered stockbroker.\\n[|Human|] How do I find a registered stockbroker in India?\\n[|AI|] You can visit the websites of National Stock Exchange (NSE) or Bombay Stock Exchange (BSE) to get a list of registered stockbrokers in India.\\n[|Human|] What documents are required to open a demat and trading account?\\n[|AI|] You will need to provide identity proof (PAN card), address proof, bank details and a passport size photograph to open a demat and trading account.\\n[|Human|] How do I start trading once I have a demat and trading account?\\n[|AI|] You can start trading by placing buy and sell orders for stocks through your stockbroker either online or offline.\\n[|Human|] How do I track my investments?\\n[|AI|] You can track your investments through your demat account. It will provide you with a consolidated view of all your investments in various stocks and other financial instruments.\\n[|Human|] '},\n",
       " {'topic': 'Will there really be any war between India and Pakistan over the Uri attack? What will be its effects?',\n",
       "  'input': \"The conversation between human and AI assistant.\\n[|Human|] I'm wondering, do you think there will be a war between India and Pakistan over the Uri attack?\\n[|AI|] It is difficult to predict the future, but tensions between India and Pakistan have been high since the attack. It is possible that the situation could escalate into a war, but it is also possible that both sides will find a peaceful resolution.\\n[|Human|] How would a war between the two countries affect the rest of the world?\\n[|AI|] A war between India and Pakistan would have global implications, particularly because both countries are nuclear powers. It could lead to a humanitarian crisis, a disruption of international trade and increase global instability.\\n[|Human|] What is the United Nations doing to prevent a war from happening?\\n[|AI|] The United Nations has expressed concerns about the situation and has called for both parties to exercise restraint and resolve their differences through diplomatic means. The international community is monitoring the situation closely and urging both India and Pakistan to engage in dialogue.\\n[|Human|] Do you think there will be any consequences for Pakistan if they do not take action against the terrorists responsible for the Uri attack?\\n[|AI|] It is possible that there could be diplomatic and economic consequences for Pakistan if they do not take action against the terrorists responsible for the Uri attack. India has accused Pakistan of harboring terrorist groups and has called for international pressure on Pakistan to take action.\\n[|Human|] \"}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open('./data/quora_chat_data.json'))\n",
    "data[:2] # dtype : list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'What is the step by step guide to invest in share market in india?',\n",
       " 'input': 'The conversation between human and AI assistant.\\n[|Human|] I want to know the step by step guide to invest in share market in India.\\n[|AI|] Sure, I can help with that. Firstly, you need to open a demat and trading account with a registered stockbroker.\\n[|Human|] How do I find a registered stockbroker in India?\\n[|AI|] You can visit the websites of National Stock Exchange (NSE) or Bombay Stock Exchange (BSE) to get a list of registered stockbrokers in India.\\n[|Human|] What documents are required to open a demat and trading account?\\n[|AI|] You will need to provide identity proof (PAN card), address proof, bank details and a passport size photograph to open a demat and trading account.\\n[|Human|] How do I start trading once I have a demat and trading account?\\n[|AI|] You can start trading by placing buy and sell orders for stocks through your stockbroker either online or offline.\\n[|Human|] How do I track my investments?\\n[|AI|] You can track your investments through your demat account. It will provide you with a consolidated view of all your investments in various stocks and other financial instruments.\\n[|Human|] '}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/laal_intern003/.cache/huggingface/datasets/json/default-3cd1d2c192d1f799/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1288.57it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 43.99it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/laal_intern003/.cache/huggingface/datasets/json/default-3cd1d2c192d1f799/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['topic', 'input'],\n",
       "        num_rows: 54456\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dump(data, open('./data/tmp.json', 'w'))\n",
    "data_r = load_dataset('json', data_files = './data/tmp.json')\n",
    "data_r # dtype: datasetdict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    './llama',\n",
    "    add_eos_token = True # add end-of-sentence token\n",
    ")\n",
    "tokenizer.pad_token_id = 0\n",
    "#data_r_mapped = data_r['train'].train_test_split(test_size = 2000, shuffle = True, seed = 42).shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/case_hold.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     case_hold \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "with open('./dataset/case_hold.pkl', 'rb') as f:\n",
    "    case_hold = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Found cached dataset lex_glue (/home/laal_intern003/.cache/huggingface/datasets/lex_glue/case_hold/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n",
      "100%|██████████| 3/3 [00:00<00:00, 515.86it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are succesfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "case_hold = build_dataset(['case_hold'], './dataset')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "new_case_hold = case_hold.shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'endings', 'label'],\n",
       "    num_rows: 45000\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_hold['train'].shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3063c37d90bcb10f1b2014a6d07f7d2022c097e80076e30e11f05761f9578a7"
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 ('legal-master')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
